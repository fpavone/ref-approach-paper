\documentclass[a4]{article}

\usepackage{amssymb,amsmath,amsthm}
\usepackage{newtxtext}
\usepackage{newtxmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\usepackage{algorithm2e}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={The reference model approach in feature selection},
            pdfauthor={Federico Pavone, Juho Piironen, Paul-Christian B\"{u}rkner, Aki Vehtari},
            pdfkeywords={keywords},
            pdfborder={0 0 0},
            breaklinks=true,
            colorlinks=true,
            linkcolor=black,
            citecolor=RoyalBlue,
            filecolor=black,
            urlcolor=RoyalBlue,
          }
\urlstyle{same}  % don't use monospace font for urls
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=american]{babel}
\else
  \usepackage{polyglossia}
  \setmainlanguage[variant=american]{english}
\fi
\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{placeins}
\usepackage{graphicx,grffile,subcaption}
\textwidth 6.0 true in
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{3}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

   
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table,dvipsnames]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}

\usepackage{soul}
\usepackage{mathtools}
\usepackage{textcomp}
\usepackage{graphicx,pdflscape}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{float}
\usepackage{supertabular}
%\usepackage{booktabs,caption,fixltx2e}
\usepackage{booktabs,fixltx2e}
\usepackage[font={small,it}]{caption}
\usepackage{tcolorbox}
\usepackage{paralist}
\usepackage{multicol}
\setcitestyle{round}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\theoremstyle{definition}
\newtheorem{example}{Example}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\N}{N}
\newcommand{\note}[1]{\textcolor{red}{#1}}

\title{Using reference models in feature selection 
	\vspace{.1in}}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Federico Pavone, Juho Piironen, Paul-Christian B\"{u}rkner and Aki Vehtari}
    
    \author{
    Federico Pavone, 
  Juho Piironen,
  Paul-Christian B\"{u}rkner
  and Aki Vehtari \footnote{Department of Computer Science, Aalto University, Finland.}
  }
	 
    
    \preauthor{\centering\large\emph}
  \postauthor{\par}
    \date{\today}
    \date{\today}
%    \predate{}\postdate{}


\begin{document}
\maketitle
\begin{abstract}
  Feature selection, or more generally, model reduction is an important aspect of the statistical workflow aiming to provide insights from data. In this paper we discuss and demonstrate the benefits of using a reference model in feature selection. A reference model acts as a noise-filter on the target variable by modeling its data generating mechanism. As a result, using the reference model predictions in the model selection procedure reduces the variability and improves stability leading to improved model selection performance. Assuming that a Bayesian reference model describes the true distribution of future data well, the theoretically preferred usage of the reference model is to project its predictive distribution to a reduced model leading to projection predictive variable selection approach. Alternatively, reference models may also be used in an ad-hoc manner in combination with common feature selection methods.  In several numerical experiments, we demonstrate superior performance of projection predictive approach and show how use of a reference model translates into better and more stable feature selection independently of the specific selection procedure. 
% These benefits are also illustrated on a real-world example of predicting the amount of body fat for which we compare a reference model based selection method to a purely data driven approach.
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In statistical applications, one of the main steps in the modelling
workflow is covariate or feature selection, which is a special case of
model reduction. Variable selection may have multiple goals.  First,
if the covariates themselves are of interest, we can use variable
selection to infer which variables contain predictive information
about the target variable.  Second, as simpler models come with the
advantages of reduced measurement costs and improved interpretability,
we may be interested in finding the minimal subset of covariates which
still provides good predictive performance (or good balance between
simplicity and predictive performance).  When the predictive
capability is guiding the selection, the true data generation
mechanism of future data can be approximated either by using the
observed data directly or alternatively by using predictions from a
\emph{reference model} \citep{vehtari2012survey}.

In data based approaches, such as Lasso selection
\citep{tibshirani1996regression} or stepwise backward/forward
regression \citep{venables2013modern,harrell2015regression}, the
observed empirical data distribution is utilised as a proxy of future
data usually in combination with cross-validation or information
criteria to provide estimates for out-of-sample predictive
performance.  In contrast, reference model based methods approximate
the future data generation mechanism using the predictive distribution
of a reference model, which can be, for example, a full-encompassing
model including all covariates.  The main assumption under any
reference model approach is that we operate in an
$\mathcal{M}$-\textit{complete} framework
\citep{book:bernardo_smith,vehtari2012survey}, that is, we have
constructed a model which reflects our beliefs about
the future data in the best possible way and which has passed model checking and criticism
\citep[see, e.g.][]{gelman2013bayesian,gabry2019visualization}.  The reference model approach
has been used in Bayesian statistics at least since the seminal
work of \citet{paper:reference_lindley}. For more historical
references, see the papers by \citet{vehtari2012survey} and
\citet{paper:model_selection}, and for most recent methodological
development see \citet{paper:projpred}.

Reference models have been also used in non-Bayesian contexts, where
\cite{harrell2015regression} describes them as full models that can be
thought of as a ``gold standard'' (for a given application).  For
example, \cite{faraggi2001understanding} deal with the necessity of
identifying interpretable risk groups in the context of survival data
using neural networks, which typically perform very well in terms of
prediction, but whose covariates are difficult to be understood in
terms of relevance.  \cite{paul2008preconditioning}, using the term
preconditioning, explore approximating models fitting Lasso or
stepwise regression against consistent estimates $\hat{y}$ of a
reference model instead of the observed responses $y$.

\subsection{Example of model selection with or without reference model}
\label{intro-example}

To motivate the further discussion and experiments, we start by simple
variable selection example using body fat data by \citet{johnson1996fitting}.
We compare projection predictive approach (\emph{projpred},
\citet{paper:projpred}) which uses a reference model, and classic
stepwise backward regression (steplm).
% \footnote{The analysis is based on \texttt{R}-notebook by Vehtari available at
%   \url{https://avehtari.github.io/modelselection/bodyfat.html}.}.
The experiments are implemented in \texttt{R} \citep{Rcore2018}.

The target variable of interest is the amount of body fat, which is
measured by immersing a person in water tank which is a complex and
expensive procedure. Additionally, we have information about 13
covariates which are anthropometric measurements (e.g., height, weight
and thicknes of skin and fat in different body locations). The
covariates are are highly correlated which causes additional
challenge in the feature selection. In total, we have 251
observations. The goal is to find the model which is able to predict
the amount of fat well while requiring least amount of
measurements for a new person.

\cite{paper:bodyfat} report results using \emph{steplm} with a significance
level of 0.157 with AIC selection \citep{akaike1974new}, fixing
abdomen and height to be always included in the model. For better
comparison we don't fix any of the variables.  The steplm approach is
carried out combining the \texttt{step} and \texttt{lm} functions.

For the selection via projpred, the Bayesian reference model includes
all the covariates using a regularised horseshoe prior~\citep{paper:rhs} on the
covariate coefficients. Submodels are explored using
forward search (the results are not sensitive to whether forward or
backward search is used), and the predictive utility is the expected
log-predictive density (elpd) estimated using approximate leave-one-out cross-validation via Pareto-smoothed importance-sampling
\citep[PSIS-LOO-CV; ][]{paper:psis_loo}.  We select the smallest  submodel with an elpd score similar to the reference model
when taking into account the uncertainty in estimating the predictive
model performance. See Appendix A for brief review of projection
predictive approach and \citet{paper:model_selection} and
\citet{paper:projpred} for more details. The complete projpred procedure is implemented in the \texttt{projpred} R package.

The inclusion frequencies of each feature in the final model given 100
bootstrap samples are depicted in Figure
\ref{fig:inclusion_frequencies}. The projpred approach has only two
variables, `abdomen' and `weight', with inclusion frequencies above
50\% (`abdomen' is the only one included always), the third most
included is `wrist' at 44\%, and the fourth is `height' at 35\%, while
steplm has seven features above 50\%. Such a higher variability and
lower stability of steplm can be observed also in the bootstrap model
selection frequencies reported in Table
\ref{tab:model_frequencies}. For example, the first five selected
models have a cumulative frequency of 76\% with projpred, but only of
14\% with steplm. In addition, the size of the selected model is much
smaller when using projpred.
\begin{figure}[tp]
  \centering
  \includegraphics[width=0.98\textwidth]{graphics/inc_prob.pdf}
  \caption{Bootstrap inclusion frequencies after 100 bootstrap samples.}
  \label{fig:inclusion_frequencies}
\end{figure}


\begin{table}[tp]
\footnotesize
\centering
\begin{tabular}{l|lr|lr}
%  \hline
M & projpred & Freq \% & steplm & Freq \%  \\ 
  \hline
1 & abdom, weight & 39 & abdom, age, forearm, height, hip, neck, thigh, wrist & 4 \\
2 & abdom, wrist & 10 & abdom, age, chest, forearm, height, neck, thigh, wrist & 4 \\
3 & abdom, height & 10 & abdom, forearm, height, neck, wrist & 2 \\
4 & abdom, height, wrist & 9 & abdom, forearm, neck, weight, wrist & 2 \\
5 & abdom, weight, wrist & 8 & abdom, age, height, hip, thigh, wrist & 2 \\
6 & abdom, chest, height, wrist & 2 & abdom, age, height, hip, neck, thigh, wrist & 2 \\
7 & abdom, biceps, weight, wrist & 2 & abdom, age, ankle, forearm, height, hip, neck, thigh, wrist & 2 \\
8 & abdom, height, weight, wrist & 2 & abdom, age, biceps, chest, height, neck, wrist & 2 \\
9 & abdom, age, wrist & 2 & abdom, age, biceps, chest, forearm, height, neck, thigh, wrist & 2 \\
10 & abdom, age, height, neck, thigh, wrist & 2 & abdom, age, ankle, biceps, weight, wrist & 2 \\
%   \hline
\end{tabular}
\caption{Bootstrap model selection frequencies after 100 bootstrap samples.}
\label{tab:model_frequencies}
\end{table}


In the first two rows of Table \ref{tab:model_performances}, we report the
predictive performances, in terms of cross-validated root mean square
error (RMSE), of the full model and the selected models using either
the projection predictive approach or the stepwise backward
elimination.  There is no difference in predictive performance of the
selected models by different approaches even if there is difference in
the selected features.

We repeat the experiment with a modified data set by adding 84
unrelated noisy features. In the last two rows of Table
\ref{tab:model_performances}, we report the cross-validated RMSE, the size of
the selected model and the number of selected noisy features using
either the projection predictive approach or the stepwise backward
elimination.  The projection predictive approach has similar
predictive performance and the same submodel size as in the first
experiment, whereas the stepwise regression has worse predictive
performance and selects a much larger submodel including a large
number of irrelevant features.

\begin{table}[tp]
\footnotesize
\centering
\begin{tabular}{ll|rrrr}
%  \hline
Data  & Method & RMSE Full & RMSE Selected & \# Selected & \# Selected noisy \\ 
  \hline
original & projpred & 4.35 & 4.47 & 2 &   \\
& steplm & 4.38 & 4.47 & 7 &  \\
\hline
+ noisy features & projpred & 4.58 & 4.54 & 2 & 0  \\
& steplm & 5.89 & 5.43 & 19 & 13 \\
%   \hline
\end{tabular}
\caption{Model predictive performances with original data (first two
  rows) and with extra noisy features (last two rows) estimated with
  10-fold cross-validation. Abbreviations: Full = full model; Selected =
  selected submodel; \# Selected = total number of features selected;
  \# Selected noisy = number of extra noisy features selected.}
\label{tab:model_performances}
\end{table}

Both projpred and steplm compare a large number of models using either
forward or backward search, which can lead to selection induced
overfitting, but even with 100 covariates, projpred is able to select a
submodel with similar performance as the full model. In this example, the two
 compared methods also differ in other aspects than the usage of a reference 
 model, such as that projpred
uses Bayesian inference and steplm uses maximum likelihood estimation. 
However, as we show in the present paper, one of the primary difference 
with respect to quality of the variable selection is indeed whether or 
not a reference model approach is applied.

\subsection{Benefits and costs of using a reference model}

A properly designed reference model is able to filter parts of the
noise present in the data, and hence provide an improved and more
stable selection process. This holds even if the reference model does
not perfectly resemble the true data generating process. Moreover, our
analyses indicate that the substantial reduction of variance
attributable to noise is usually more important than small potential
bias due to model misspecification.  We argue that, regardless of how
the reference model is set up and used in the inference procedure, it
can be always seen as acting as a filter on the observed
data. Furthermore, regardless of what specific model selection method
is used, a reference model can be used instead of raw data during the
selection process to improve the stability and selection
performance. Our results indicate that the core reason why the
reference model based methods perform well is the reference model
itself, rather than the specific way of using it.  In general, the
less data we have and the more complex the estimated models, the
higher the benefit of using a reference models as the basis for
feature selection.

If one of the models to be compared is the full model which can be
used as a reference model, there is no additional cost of using a
reference model. Sometimes including all the available features in an
elaborate model can often be computationally demanding.  In such a
case, even simpler screening or dimensionality reduction techniques,
as for example the supervised principal components
\citep{bair2006prediction,piironen2018} can produce useful reference
models.

The remainder of the paper is structured as follows. In Section
\ref{reference-model-approach}, we review the concept of the reference
model, its benefits with examples and how it can be used as a filter
on data in a simple way. In Section \ref{comparison-minimal-subset} and 
Section \ref{comparison-complete-subset}, we show the
benefits of a reference model approaches for minimal and complete 
feature selection, respectively, before we illustrate the use 
of a reference model in a detailed case study of the 'body fat' data.
Finally, we end with a conclusion in Section \ref{conclusion}.


\hypertarget{reference-model-approach}{%
\section{Why the reference model helps}\label{reference-model-approach}}
 
A good predictive model is able to filter part of the noise present in
the data. The noise is the main source of the instability and tends to
obscure the relevance of the covariates in relation to the target
variable of interest. Let us consider the following simple
explanatory example taken from \cite{paper:projpred}. The data
generation mechanism is
\begin{alignat}{2} \label{eq:simulated_data}
     &f\sim \N(0,1) && \nonumber \\ 
     Y|&f\sim \N(f,1) && \\
     X_{j}|&f \overset{iid}{\sim} \N(\sqrt{\rho}f,1-\rho) \quad &&j=1,\ldots,k \nonumber \\
     X_{j}|&f \overset{iid}{\sim} \N(0,1) &&j=k+1,\ldots,p \nonumber,
\end{alignat}
where $f$ is the latent variable of interest of which $Y$ is a noisy
observation. The first $k$ covariates are strongly related to the
target variable $Y$ and correlated among themselves. Precisely, $\rho$
is the correlation among any pair of the first $k$ covariates, whereas
$\sqrt{\rho}$ and $\sqrt{\rho/2}$ are the level of correlation between
any relevant covariate and, respectively, $f$ and $Y$. If we had an
infinite amount of observations, the sample correlation would be equal
to the true correlation between $X_j$ and $Y$. However, even in this
ideal asymptotic regime, this correlation would still remain a biased
indicator of the true relevance of each covariate (represented by the
correlation between $X_j$ and $f$) due to the intrinsic noisy nature
of $Y$.

When using a reference model, we first obtain predictions for $f$
using all the covariates $\{X_{j}\}_{j=1}^{p}$ taking into account
that we have only observed the noisy representation $Y$ of $f$. If our
model is good, we are able to describe $f$ better than $Y$ itself can,
which improves the accuracy in the estimation of the relevance of the
covariates.  Figure \ref{fig:correlation} illustrates this process in
the form of a scatter plot of (absolute) correlations of the
covariates with $Y$ against the corresponding correlations with the
predictions of a reference model (in this case the posterior
predictive means of model \eqref{eq:ref_mod}; see Section
\ref{simulations}). Looking at the marginal distributions, we see
that using a reference model to filter out noise in the data, the two
groups of features (relevant and non-relevant) can be distinguished
much better than when the correlation is computed using the observed
noisy data directly.

\begin{figure}[tp]
  \centering
  \vspace{-5mm}
  \includegraphics[width=0.4\textwidth]{graphics/correlation.pdf}
  \caption{Sample correlation plot of each feature (relevant in red, 
  non-relevant in blue) with the target variable $y$ and the latent variable 
  $f$ respectively on the x and y axis. Data are generated according to 
  procedure \eqref{eq:simulated_data} with parameters $n=70$, $\rho=0.3$,
  $k=100$, $p=1000$.\\}
  \label{fig:correlation}
\end{figure}

\hypertarget{comparison-minimal-subset}{
\section{Minimal subset feature selection}\label{comparison-minimal-subset}}

In the bodyfat example above, we desired at the same time good
predictive performance and selection of a smaller number of
features. When the goal is to select a minimal subset of features,
which have similar predictive performance as the full model, we call it
\emph{minimal subset feature selection}. This minimal subset might
exclude features which have some predictive information about the
target, but given the minimal subset these features are not able to
provide such additional information that would improve predictive
performance. The usual reason for this is that the relevant features
not in the minimal are subset highly correlated with features already in
the minimal subset. We will return to a problem of finding all
the features with some predictive power in Section~\ref{complete-selection}.

Using the data generating mechanism \eqref{eq:simulated_data}, we
simulate data sets of different sizes with a relatively large number
of features $p=100$ with $k=20$ of them being predictive.  We compare
the minimal subset feature selection perfomance of projection
predictive approach (with a reference model), a Bayesian stepwise
forward selection with and without a reference model, and maximum
likelihood stepwise forward selection with and without a reference
model:
\begin{itemize}
\item projpred: The projective prediction approach is used. 
  The reference model is a Bayesian linear regression
  model using the first five supervised principal components \citep{piironen2018} as predictors
  and the full posterior predictive distribution as the basis of the projection.   
  The search heuristic  is forward search and the predictive performance 
  is estimated via 10-fold cross-validation.
  
\item Bayesian stepwise selection (without a reference model): At each step, the fitted model
 is a Bayesian linear regression using the regularised horseshoe
  prior and the feature excluded is the one with the highest Bayesian
  $p$-value defined as $\text{min}\{P(\theta\leq0 \; | \; D),P(\theta>0 \; | \; D)\}$.
The selection continues if the reduced model has an elpd score higher 
(i.e., better) than the current model.

\item Bayesian stepwise selection (with a reference model): The reference model is the same as for projpred, but only point predictions (posterior predictive means) $\hat{y}$ are used to replace the target variable $y$. The same Bayesian
  $p$-value selection strategy as in the data based Bayesian stepwise selection is used.

%Same as the data based Bayesian stepwise selection, but instead of using the observed data values to fit the models at each step, it uses the target values predicted by the reference model (which is the same as in the projpred approach).
\item steplm (without a reference model): Stepwise selection using AIC as in the bodyfat example.

\item steplm (with a reference model): The reference model is the same as for
 projpred, but only point predictions (posterior predictive means) $\hat{y}$ 
 are used to replace the target variable $y$. The same AIC selection strategy 
 as in the data based steplm is used.

\end{itemize}

In Figure \ref{fig:rmse_vs_fdr}, the predictive performance is shown
for different values of $n$ and $\rho$ in terms of RMSE and the false discovery rate (FDR, the ratio of the number of non-relevant selected
features over the number of selected features) of the selected submodel,
averaged after 100 data simulations. We see that projpred has superior FDR and the
smallest RMSE. Using a reference model improves steplm significantly,
but the minimal subset feature selection stays unreliable. Stepwise
Bayesian linear regression model without a reference model is better
than steplm without a reference model, but the minimal subset feature
selection is unreliable.

When the variable selection is repeated with different simulated data
sets, there is some variability in which variables are selected. We
measure the stability of variable selection by computing the entropy
of observed distribution over different models. The smallest entropy
would be obtained if the approach always selects the same set of
variables, and the largest entropy would be observed if the approach
would always select different set of variables.
Figure \ref{fig:entropy} reports the entropy scores for the different compared
methods. Lower entropy corresponds to a more stable
selection. Highly correlated predictive features may happen to be
alternately selected, making the measure of the stability of the
selection a non-trivial task. Entropy cannot distinguish the interchangeability 
due to correlation from instability. Thus, such a measure should be 
considerd as a relative and not as an absolute measure of stability.
The use of a reference model
improves slightly the stability of steplm in feature selection. The
projection predictive approach results to be far more stable than all
other approaches. This is likely due to projpred being based on better
decision theoretical formulation which 1) takes into account the full
predictive distribution and not just point estimate and 2) projects
the reference model posterior to the submodel instead of using a
simple refit of submodels.

% TODO: Bigger and more distinctive symbols in figures
% label order: steplm, bayes.step, projpred
\begin{figure}[tp]
  \centering
  \includegraphics[width=0.98\textwidth]{graphics/rmse_vs_fdr_parallel.pdf}
  \caption{Root mean square error (RMSE) against false discovery rate in the minimal subset feature selection.}
  \label{fig:rmse_vs_fdr}
\end{figure}

% TODO: Bigger and more distinctive symbols in figures
% TODO: order steplm, bayes.step, projpred
\begin{figure}[tp]
  \centering
  \includegraphics[width=0.98\textwidth]{graphics/entropy_parallel.pdf}
  \caption{Entropy score in the minimal subset feature selection.}
  \label{fig:entropy}
\end{figure}



% \hypertarget{comparison}{%
%   \section{A comparison in the normal means problem framework}\label{comparison}}
\section{Complete feature selection}\label{comparison-complete-subset}
\label{complete-selection}

An alternative to minimal subset feature selection is \emph{complete
  feature selection} in which the goal is to find all relevant
features that have some predictive information about the target. In
complete feature selection, it is possible that there are theoretically
relevant features, but given finite noisy data we are not able infer their relevance.
The projection predictive approach was originally designed for the minimal subset
feature selection, but we generalize it here for the complete
feature selection case. In addition, we analyse the benefits of the reference
model approach in combination with three other complete feature
selection approaches. 

As criteria of selection performance, we evaluate the average false
discovery rate and the average
sensitivity (i.e., the ratio of the number of relevant selected
features over the total number of relevant features). We also provide
a comparison of the stability of the selection by means of a stability
measure proposed by \citet{paper:stability}. 

%Table \ref{tab:comparison} summarises the methods used in the numerical experiments.

\subsection{Iterative projections}

We can use the projection predictive approach for complete feature
selection by using it iteratively.  Applying the straightforward
implementation of projpred, we are able to select a minimal subset of
features, which yield to a model with a predictive performance
comparable to the full model one.  The iterative projection repeats
the projpred selection for different iterations, at each time
excluding from the search the features selected in the previous
iterations. At each iteration the selected submodel size corresponds to the
one having a predictive performance close enough to the baseline
model, which in this iterative version is the submodel with the
highest predictive score explored at the current iteration. The
stopping rule that we adopted is the following:
\begin{equation} 
\label{eq:rule_of_thumb}
\text{min} \{i\in \{0,\ldots,p\}: \text{P}(\text{elpd}_{i}-\text{elpd}_{\text{best}}>0)\geq \alpha \},
\end{equation}
where $i$ indexes the submodel size and ``best'' stays for the best predictive explored submodel
at the considered iteration. The algorithm terminates when the empty model (only intercept) satisfies
the stopping rule (see Algorithm \ref{alg:iterated_proj}).
The choice of the hyperparameter $\alpha$ can be non-trivial, we have 
observed sensitivity of the selection to such a choice mainly when using cross-validation with a small
number of observations or not very predictive features. 
In our experiments we chose the default value used in the \texttt{projpred} 
\texttt{R}-package that corresponds to $\alpha=0.16$.

\begin{center}
\begin{algorithm}[H]
\SetAlgoLined
\KwResult{$R:=\{\text{selected features}\}$}
 $F:=\{\text{set of features}\}$ \;
 $R=\{\emptyset\}$ \;
 Fit reference model\;
 \While{$F\neq\{\emptyset\}$}{
   projection.HeuristicSearch \;
   projection.elpdEstimate() \;
%:
   $S=\text{min}\{\text{sub}: P(\text{elpd}_{\text{sub}}-\text{elpd}_{\text{base}}>0)\geq\alpha)$\} \;
  
  \eIf{$S=\{\emptyset\}$}{
   break \;
   }{
   $R=R\cup S$\;
   $F=F\setminus S$\;
  }
 }
 \caption{Automated iterative projections}
 \label{alg:iterated_proj}
\end{algorithm}
\end{center}

\subsection{Alternative complete feature selection approaches}

% Many Bayesian approaches for complete feature selection first fit a
% Bayesian regression model using some sparsifying prior on the
% regression coefficients, for example, as discussed in
% \cite{paper:dirichlet_laplace}, \cite{bhadra2017horseshoe+} and
% \cite{johnstone2004needles}.  In our experiments, we include the
% Dirichlet-Laplace \cite[DL;][]{paper:dirichlet_laplace} prior and the
% regularised horseshoe (RHS) prior \citep{paper:rhs}, which in our experience
% scales better to higher number of covariates as compared to other
% horseshoe-like priors.
% \note{Aki: How this paragraph connects to the next paragraph?}

% If these priors have continuous support, as is the case for the DL and
% horseshoe-like priors \citep{paper:hs,bhadra2017horseshoe+,paper:rhs},
% there is no canonical way to complete the selection since the
% posterior probability of an exactly zero signal is always
% zero. Whereas \cite{paper:dirichlet_laplace} used k-means clustering
% over the posterior samples to perform the actual selection,
% \cite{bhadra2017horseshoe+} just focused their analysis on the
% obtained shrinkage and the correct estimation of the sparse signals
% without any actual selection.

% For the Bayesian regression
% models with these priors, we compare the posterior shrinkage and the
% correct estimation of the sparse signals with and without using a
% reference model as a noise filter. For this purpose, we consider the
% average SSE (sum of square errors) and the average SESE (sum of
% expected square errors) as quantities measuring estimation
% accuracy. They are defined as \
% \begin{align}
% \text{SSE}&=\frac{1}{n-3}\sum_{j=1}^{p}(\hat{\theta}_{j} - \theta^{0}_{j})^{2} \label{eq:SSE} \\
% \text{SESE}&=\frac{1}{n-3}\sum_{j=1}^{p}\int_{\mathbb{R}}(\theta_{j}-\theta^{0}_{j})^{2}p(\theta_{j}|z_{1},\ldots,z_{p})d\theta_{j} \\
% &=\frac{1}{n-3}\sum_{j=1}^{p}\mathbb{E}_{\theta_{j}}[(\theta_{j}-\theta^{0}_{j})^{2}|z_{1},\ldots,z_{p}], \label{eq:SESE}
% \end{align}
% where $\theta_{j}^{0}=\sqrt{n-3}\,T_{F}(\rho_{j})$ is the true value,
% $\hat{\theta}_{j}$ is the posterior median of each parameter
% $\theta_{j}$. We divide the errors by $(n-3)$ to have quantities
% independent of the number of statistical units.  The SESE takes the
% posterior uncertainty in account and thus also the shrinkage of the
% posterior distributions, whereas the SSE only measures the bias in the
% point estimates.  Any (sensible) point estimate could be used to
% compute the SSE but here we follow \cite{paper:dirichlet_laplace} and
% use the posterior median. The results that we show later on in Section
% \ref{shrinkage-signal} are not sensitive to the choice of this point
% estimate.

% For shrinkage priors, one central aspect is the amount of shrinkage
% they actually impose on the relevant and irrelevant features. Ideally,
% the irrelevant features should be completely shrunk to zero while the
% relevant features should not be shrunk at all. The amount of shrinkage
% can be quantified by the shrinkage factor $\kappa_{j}$ for each
% parameter $\theta_{j}$ ranging from $0$ (no shrinkage) to $1$
% (complete shrinkage). For model \eqref{eq:normal_means_problem2} with
% a hierarchical shrinkage prior of the type
% \begin{align}
% Z_{j}|&\theta_{j}\overset{ind}{\sim}N(\theta_{j},1) \qquad j=1,\ldots,p \\
% &\theta_{j}|\tau,\nu_{j}\overset{ind}{\sim}N(0,\nu_{j}^{2}\tau^{2})
% \end{align}
% the shrinkage factor $\kappa_{j}$ is defined as
% \begin{equation}
% \kappa_{j}=\frac{1}{1+\tau^{2}\nu_{j}^{2}}.
% \end{equation}
% \
% For RHS and DL priors we have
% \begin{equation}
% \kappa_{j}^{\text{RHS}}=\frac{1}{1+\tau^{2}\lambda_{j}^{2}}, \qquad \kappa_{j}^{\text{DL}}=\frac{1}{1+\psi\phi_{j}^{2}\tau^{2}}.
% \end{equation}
% In the left hand-side expression, $\tau$ and $\lambda_{j}$ represent
% the global and local shrinkage parameters of the regularised horseshoe
% prior \cite[for further details see][]{paper:rhs}, whereas on the
% right hand-side, $\psi\phi_{j}^{2}\tau^{2}$ is the variance term of
% the Dirichlet-Laplace prior of parameter $\theta_{j}$ following the
% notation of \cite{paper:dirichlet_laplace}. We use the posterior mean
% of $\kappa_{j}$ as a point estimate.

%Within the set of complete selection procedures, we consider three
%Bayesian methods.  \cite{johnstone2004needles} use a spike-and-slab
%type prior given by a mixture of a delta distribution in zero and a
%heavy-tailed distribution \cite[see
%also][]{paper:spike_slab_mitchell}. When the latter is a Laplace
%distribution, they show an interesting thresholding property of the
%median estimator and, based on this observation, present a complete
%selection procedure using their prior. We refer to this method as
%\emph{EB.med}. Further, we use a method to control for the local false
%discovery rate (\emph{loc.fdr}) as described in \cite{paper:efron,
%  efron2012large}. Finally, we device a simple selection procedure
%based on posterior credible intervals using the regularised horseshoe
%prior in which we select covariates whose 90\% credible intervals
%excludes zero (\emph{ci.90}) \note{Reference for this?}.

We consider three alternative complete selection procedures,
 the control of the local
false discovery rate \citep{paper:efron, efron2012large}, the
empirical Bayes median \citep{johnstone2004needles}, and the selection
by posterior credible intervals.

The control of the local false discovery rate consists of testing the
z-values $\{z_{j}\}_{j=1}^{p}$ on whether they belong to the
theoretical null distribution $f_{0}$ (i.e., the null hypothesis $H_0$
meaning no relevance) against the alternative hypothesis distribution
$f_{1}$. In our case $f_{0}$ corresponds to the standard normal
distribution (see expression \eqref{eq:normal_means_problem2}). The
quantity of interest is the local false discovery rate (loc.fdr)
defined as: \
\begin{equation}
\text{loc.fdr}(z)=P(H_{0}|z)=\frac{f_{0}(z)\pi_{0}}{f(z)},
\end{equation}
where $\pi_{0}$ is the prior probability of $H_0$ and
$f(z)=\pi_{0}f_{0}(z)+(1-\pi_{0})f_{1}(z)$ is the marginal
distribution of the z-values. The latter is estimated using splines
with 7 degrees of freedom. We select features with local false
discovery rate below $0.2$, which is suggested by
\citet{efron2012large} as it corresponds to a Bayes factor larger than
36 (assuming $\pi_{0}\geq0.9$). The results of the comparison are not
sensitive to the specific value. To estimate $\pi_{0}$ from the data,
we use the default setting provided by the \texttt{R}-package
\texttt{locfdr} \citep{Efron+Turnbull+Narasimhan:2015:locfdr}.

The empirical Bayes median procedure consists of fitting a Bayesian
model with a prior composed by a mixture of a delta spike in zero and
a heavy-tailed distribution. We use the implementation in the
\texttt{R}-package \texttt{EbayesThresh}
\citep{Silverman+etal:2017:EbayesThresh}. As suggested by
\cite{johnstone2004needles}, we use a Laplace distribution resulting
in a thresholding property, that is, there exists a threshold value
such that all the data under that threshold have posterior median
equal to zero. Therefore the selection is done by selecting only those
parameters whose posterior median is different from zero. The
hyperparameter of the Laplace distribution and the mixing weight of
the prior are estimated by marginal maximum likelihood.

The selection
by 90\% posterior credible intervals is done using the regularised
horseshoe prior and selecting those features whose posterior
distribution does not include zero in the interval between the 5\% and
the 95\% quantiles (cite).

All of these methods provide a complete selection procedure and we
compare their performance with and without using a reference model.
That is, in the data condition, we apply the method on the original
data $y$ while, in the referece model condition, we replace $y$ by
their mean predictions $\hat{y}$ based on the reference model.
  

%\note{Aki: ``Type: Complete selection'' doesn't make sense in the table. Maybe we need to discuss when to use the term complete selection and what is the alternative title for this section.}

% \begin{table}[tp]
% \scriptsize
% \centering
% \begin{tabular}{l|l|l|l}
% Method & Abbreviation & Type & Quantities of interest \\ 
%  \hline
% Regularised Horseshoe & RHS & Shrinkage prior & SSE, SESE, $\kappa$ \\
% Dirichlet-Laplace & DL & Shrinkage prior & SSE, SESE, $\kappa$ \\
% Local false discovery rate & loc.fdr & Complete selection & fdr, sensitivity, stability \\
% Median estimator & EB.med & Complete selection & fdr, sensitivity, stability \\
% Credible intervals & ci.90 & Complete selection & fdr, sensitivity, stability \\
% \end{tabular}
% \caption{Summary of the shrinkage and selection methods used in the numerical experiments.}
% \label{tab:comparison}
% \end{table}

\subsection{Simulations}
\label{simulations}

To investigate the performance of the four complete selection procedures,
we are going to use simulations based on the normal means problem.
The normal means problem consists of estimating the (usually
sparse) vector of means of a vector of normally distributed
observations. The dimensionality of the vector of means is denoted by
$p$ and $\{z_{j}\}_{j=1}^{p}$ is the vector of observations of the
random variables $\{Z_{j}\}_{j=1}^{p}$. The task is to estimate the
latent variables $\{\theta_{j}\}_{j=1}^{p}$ of the following model: \
\begin{equation}\label{eq:normal_means_problem}
Z_{j}|\theta_{j},\sigma\overset{ind}{\sim}\N(\theta_{j},\sigma^{2}), \quad j=1,\ldots,p.
\end{equation}
This is equivalent to a linear regression where the design matrix is
the identity matrix with the number of features being equal to the
number of observations. This formulation can be found in practice, for
example, in the analysis of microarray data, where a large set of
genes are tested in two groups of patients labeled as positive or
negative to some disease \citep{paper:efron,efron2012large}. The
objective of the analysis is to select the subset of genes
statistically relevant to the disease. One common way to proceed is to
compute the two-sample $t$-statistic for every gene separately.  After
normalising these statistics, they then become the data $Z$ in the
normal means problem \eqref{eq:normal_means_problem}. For further
details see the examples by \cite{paper:efron, efron2012large}.

In our experiments, we retrieve the normal means problem from the
sample correlations between the target and the covariates using the
Fisher $z$-transformation \citep{hawkins1989using}. Suppose we have a
continuous target random variable $Y,$ a set of $p$ continuous
covariates $\{X_{j}\}_{j=1}^{p}$, and denote
$\rho_{j}=\Cor(Y,X_{j})$. Further, suppose we have observed $n$
statistical units and define $r_{j}$ the sample correlation between
the observations of the target variables $\{y_{i}\}_{i=1}^{n}$ and the
$j$-th covariate $\{x_{ij}\}_{i=1}^{n}$. Finally, we refer to the
Fisher $z$-transformation function $\text{tanh}^{-1}(\cdot)$ as
$T_{F}(\cdot)$. Assuming each pair $(Y,X_{j})$ to be bivariate
normally distributed, the corresponding transformed correlations are
approximately normally distributed with known variance: \
\begin{equation} \label{eq:fisher_transformation}
T_{F}(r_{j})\overset{ind}{\sim} \N(T_{F}(\rho_{j}),\frac{1}{n-3}), \quad j=1,\ldots,p.
\end{equation}
Therefore, rescaling the quantities $T_{F}(r_{j})$ by $\sqrt{n-3}$ and
denoting the results as $z_{j}$, we have the formulation
\eqref{eq:normal_means_problem} of the normal means problem, this time
with unit variance: \
\begin{equation} \label{eq:normal_means_problem2}
Z_{j}|\theta_{j}\overset{ind}{\sim}\N(\theta_{j},1), \quad j=1,\ldots,p.
\end{equation}
In this case, the quantities of interest $\theta_{j}$ are equal to
$\sqrt{n-3}\,T_{F}(\rho_{j})$.

We use different levels of correlation $\rho\in\{0.3,0.5\}$ and numbers of
observations $n\in\{50,70,100\}$. The total number of covariates $p$
and the number of relevant covariates $k$ are fixed to $p = 1000$ and
$k = 100$, respectively. In general, the lower $\rho$ and $n$, the
more challenging the variable selection is. For this example, \citet{paper:projpred} proposed to
use a reference model which a Bayesian linear regression using the first five
supervised principal components (SPC) as covariates and imposing an hierarchical prior on
their coefficients:
\begin{equation}
\label{eq:ref_mod}
\begin{aligned}
    Y_{i}|&\boldsymbol{\beta},\sigma^{2},\boldsymbol{u}_{i} \overset{ind}{\sim} \N(\boldsymbol{u}_{i}^{T}\boldsymbol{\beta},\sigma^{2}) \quad &i=1,\ldots,n \\
    &\beta_{j}|\!\begin{aligned}[t] &\tau \overset{iid}{\sim} \N(0,\tau^{2})\\
    &\tau \sim t_{4}^{+}(0,s_{max}^{-2}) 
    \end{aligned} &j=1,\ldots,5 \\ 
    &\sigma \sim t_{3}^{+}(0,10). \\
\end{aligned}
\end{equation}
In the above, $u_{ij}$ represents the $j$-th SPC evaluated at
observation $i$, and $s_{max}$ denotes the sample standard deviation
of the largest SPC.  The SPCs are computed using the
\texttt{R}-package \texttt{dimreduce} setting the screening threshold
parameter at $0.6s_{max}$.  In our experiments, the results are not
sensitive to the specific choice of the screening threshold, yet a more
principled approach would be to use cross-validation to select the
threshold as done in \cite{paper:projpred}.

% The normal means problem formulation is obtained as explained in
% Section \ref{comparison} by applying the Fisher-z-transformation to
% the empirical correlations. We label the reference approach as
% ``ref'', which consists of using the reference model on top of the
% computation of the z-values, and the standard approach based on the
% observed data simply as ``data''. In Section \ref{shrinkage-signal},
% we analyse the different shrinkage and signal estimates in the context
% of the full Bayes estimation methods by comparing the ref and data
% approach. In Section \ref{complete-selection}, we present results for
% the complete selection procedures.

% \hypertarget{shrinkage-signal}{%
% \subsubsection{Shrinkage and signal analysis}\label{shrinkage-signal}}
% In the framework of fully Bayesian estimation methods, we analyse the
% effect of the reference model using two continuous hierarchical
% shrinkage priors: the Dirichlet-Laplace
% \citep{paper:dirichlet_laplace} and the regularised horseshoe prior
% \citep{paper:rhs}.
% %
% For the Dirichlet hyperparameter of the Dirichlet-Laplace prior, we
% use an uniform prior on the interval $(1/p,1/2)$. For the regularised
% horseshoe prior, we follow the recommendations provided in
% \cite{paper:rhs} by setting the prior guess of the number of relevant
% covariates to one in order to achieve the highest possible
% shrinkage. The results are not sensitive with respect to the specific
% choice.  \ Since, as explained at the start of Section
% \ref{comparison}, continuous priors usually do not provide an
% automatic selection procedure, we study the amount of shrinkage and
% the bias of the posterior medians looking at the SESE and the SSE (see
% definitions \eqref{eq:SSE} and \eqref{eq:SESE}), together with the
% posterior mean of the shrinkage factors.

% \begin{figure}[tp]
%   \centering
%   \includegraphics[width=0.98\textwidth]{graphics/post_int.pdf}
%   \caption{Posterior intervals for the parameters of the normal means
%     problem ($n=50$, $\rho=0.3$) using the regularised horseshoe
%     prior. Posterior means and one standard deviation intervals are
%     depicted. Relevant and non-relevant features are displayed in red
%     and in blue respectively. Horizontal dashed lines corresponds to
%     the true value of the mean for the relevant features.\\}
%   \label{fig:posterior_intervals}
% \end{figure}

% Figure \ref{fig:posterior_intervals} shows an example of posterior
% distributions for the model \eqref{eq:normal_means_problem2} using the
% regularised horseshoe prior with the data approach on the left and the
% ref approach on the right-hand side. It is evident how the reference
% model helps in tearing apart the signal from the noise, resulting in
% less biased estimates and clearer separation between relevant and
% irrelevant features. This is quantified in Figure \ref{fig:SESE_SSE}
% where average SESE and SSE are depicted based on 100 simulations. We
% observe that the reference approach (in orange) has a clearly lower
% estimate for both the SESE and the SSE. As $n$ and $\rho$ increase,
% the amount of error diminishes, as expected, due to a larger amount of
% information of the relevance of each variable provided by the
% data. The two priors (RHS and DL) seem to achieve similar results
% under all conditions.

% \begin{figure}[tp]
%   \centering
%   \includegraphics[width=0.98\textwidth]{graphics/SESE_SSE.pdf}
%   \caption{Average SESE and SSE with one standard deviation intervals based on 100 simulations.\\}
%   \label{fig:SESE_SSE}
% \end{figure}

% Figure \ref{fig:k} shows the posterior means for the shrinkage factors
% of each parameter using the regularised horseshoe prior, averaged over
% 100 data simulations. Relevant and non-relevant features are
% highlighted in red and blue respectively. The ideal shrinkage would be
% $\kappa=0$ when the parameter is a signal and $\kappa=1$ when it is
% noise. The shrinkage factors using the data and reference approach are
% depicted on the y-axis and on the x-axis respectively. If the
% reference model did not make any difference, we would expect the
% points to lie on the diagonal. For the non-relevant features points
% are on the diagonal on average for all the simulated scenarios, which
% implies that their shrinkage was not improved by the reference
% approach. For the relevant features, the points are always located
% under the diagonal, which means that the reference model helps and
% true signals are shrunk less. This benefit is roughly proportional to
% the difficulty of the problem, that is, the benefit increases with
% lower correlations and a smaller number of observations. The perfect
% distinction of the two clusters of features (relevant and
% non-relevant) observable on both the marginal distributions of the
% shrinkage factors is due to the averaging over the 100 data
% simulations. In a single data realisation, there is more noise and the
% cluster are closer and overlapping. The results for the
% Dirichlet-Laplace prior are very similar.

% \begin{figure}[tp]
%   \centering
%   \includegraphics[width=0.98\textwidth]{graphics/k_RHS.pdf}
%   \caption{Average posterior mean values for the shrinkage factors
%     using the regularised horseshoe prior based on 100
%     simulations. Relevant and non-relevant variables are displayed in
%     red and blue respectively.\\}
%   \label{fig:k}
% \end{figure}

Figure \ref{fig:sensitivity_vs_fdr_iterated} shows the average sensitivity on
the vertical axis and the average false discovery rate on the
horizontal axis based on 100 data simulations for the different
combinations of $n$ and $\rho$. The best selection performance is on the
top-left corner of each plot, as it implies the lowest false discovery
rate and the highest sensitivity. We see that regardless of the
applied selection method, the use of a reference model improves the
selection performance, as it reduces the false discovery rate (shifting
to the left) or increases the sensitivity (shifting upwards). In
accordance with what can be expected, the larger data set size ($n$) and the higher the true correlations ($\rho$), the easier the
selection is. Thus, for easier selection scenarios, the benefits of
the reference model are smaller since the raw data already provide
enough information to identify the relevant covariates. The iterative
projection predictive approach has good false discovery rate in all cases, and the sensitivity is good except when the number of observations and the correlation level
are small. Tuning $\alpha$ might lead to improved selection, yet
the main source of poor performance could well be the miscalibration of the uncertainty of the predictive
utility score for misspecified models with cross-validation \citep{bengio2004no}.

\begin{figure}[tp]
  \centering
  \includegraphics[width=0.98\textwidth]{graphics/sensitivity_vs_fdr_iterated.pdf}
  \caption{Sensitivity against false discovery rate based on 100 data simulations.\\}
  \label{fig:sensitivity_vs_fdr_iterated}
\end{figure}

%\begin{figure}[tp]
%  \centering
%  \includegraphics[width=0.98\textwidth]{graphics/sensitivity_vs_fdr.pdf}
%  \caption{Sensitivity against false discovery rate based on 100 data simulations.\\}
%  \label{fig:sensitivity_vs_fdr}
%\end{figure}

Figure \ref{fig:stability_iterated} shows the estimates of the stability
measure proposed by \cite{paper:stability} with 0.95 confidence
intervals based on 100 simulations. Such a measure takes in account
the variability of the subset of the selected features at each
simulation (originally at each bootstrap sample), modelling the
selection of each variable as a Bernoulli process. Further details are
available in \cite{paper:stability}. The reference model helps in
improving the stability of the selection: Again, the benefits are
bigger when the problem is more difficult (small $n$ and $\rho$). In
addition, we observe less uncertainty in the stability estimates for
the reference approach (i.e., smaller width of the 95\% intervals),
which can be still connected to the overall stability of the
procedure. As in Figure \ref{fig:sensitivity_vs_fdr_iterated}, the iterative
projection does not perform well in the hardest scenarios. 

\begin{figure}[tp]
  \centering
  \includegraphics[width=0.98\textwidth]{graphics/stability_iterated.pdf}
  \caption{Stability estimates with 95\% intervals based on 100 data simulations.\\}
  \label{fig:stability_iterated}
\end{figure}


\hypertarget{real-world-data}{%
\section{Case study: Body fat data}\label{real-world-data}}

We conclude our experiments using the body fat dataset one
more time, which was already described in Section \ref{intro-example}. We
carry out two different analyses: one using the normal means problem
framework and another one using selection via stepwise backward
regression. In the former, we add noisy uncorrelated covariates to the
original data up to a total of 1000 features. These covariates are
normally distributed and scaled as the original covariates. We compute
correlations between each variable and the target variable, that is
the amount of fat, and transform them by Fisher-Z-transformation. The
original assumption in order for \eqref{eq:fisher_transformation} to
hold is that the variables are jointly normally distributed. In our
experience the normal approximation in
\eqref{eq:fisher_transformation} is still reasonable, but after
rescaling by $\sqrt{n-3}$ we do not fix the variance to be one, and
instead estimate it from the data. The methods we compare are those
used in Section \ref{complete-selection}: the control of the local
false discovery rate (loc.fdr), the empirical Bayes median (EB.med)
and the selection by posterior credible intervals at level 90\%
(ci.90).  In order to vary the difficulty of the selection, we
bootstrap subsamples of different sizes, going from $n=50$ up to
$n=251$ (i.e., the full size of the data). For each condition, results
are averaged over 100 bootstrap samples of the respective size.

\begin{figure}[tp]
  \centering
  \includegraphics[width=0.98\textwidth]{graphics/bodyfat_sensitivity_vs_fdr.pdf}
  \caption{Sensitivity against false discovery rate based on 100 bootstrap samples.\\}
  \label{fig:bodyfat_sensitivity_vs_fdr}
\end{figure}

\begin{figure}[tp]
  \centering
  \includegraphics[width=0.98\textwidth]{graphics/bodyfat_stability.pdf}
  \caption{Stability estimates with 0.95 confidence intervals based on 100 bootstrap samples.\\}
  \label{fig:bodyfat_stability}
\end{figure}

Figure \ref{fig:bodyfat_sensitivity_vs_fdr} shows the sensitivity
against the false discovery rate. Since we do not have a ground truth
available with regard the original covariates of the data, we believe
it is reasonable to consider all of them relevant, at least to some
degree. The artificially added features are naturally irrelevant by
construction. In almost all of the bootstrapped subsamples, the reference
model improves the selection both in terms of sensitivity and false
discovery rate. When $n=50$, we observe worse false
discovery rates, yet by a lower amount compared to the gain in
sensitivity. Again, we observe that the benefits are more evident as
the selection becomes more challenging (i.e., lower number of
observations). Figure \ref{fig:bodyfat_stability} shows the stability
results using the measure by
\cite{paper:stability}. The benefits of the reference model are here
marginal with only small improvements.

\begin{figure}[tp]
  \centering
  \includegraphics[width=0.98\textwidth]{graphics/bodyfat_step_refvsdata.pdf}
  \caption{Stepwise backward selection with and without using a reference model. The x-axis denotes the number of selected irrelevant features on the left and the out-of-sample RMSE on the right-hand side based on 100 bootstrap samples.\\}
  \label{fig:bodyfat_step_refvsdata}
\end{figure}

In the second analysis, we repeat the selection of Section
\ref{intro-example} via stepwise backward regression. In this case, the
overall number of covariates (original plus noisy) is 100, as it was
in the last part of Section \ref{intro-example}. We compare results with
and without using a reference model on top of the procedure following
our simple reference model approach outlined in
\eqref{eq:ref_mod}. Figure \ref{fig:bodyfat_step_refvsdata} shows the
number of irrelevant features included in the final model and the
out-of-sample root mean square error (RMSE). Results are based on 100
bootstrap samples on the whole dataset, and the predictive performance
is tested on the observations excluded at each bootstrap sample. We
observe that the reference model reduces the number of irrelevant
features included in the final model. This leads to less overfitting
and thus to improved out-of-sample predictive performance in terms of
RMSE. The reference model approach applied to the stepwise backward
regression achieves outstanding improvements considering its
simplicity, yet it does not reach the goodness of the much
more sophisticated projective prediction approach (see
results of Section \ref{intro-example}).

In this example, we have used the reference model defined as a linear
regression over some supervised principal components, because it is
natural for a large number of correlating variables, has fairly good
predictive performance and is computationally efficient. We do not
argue that this is always the best choice and more sophisticated
models can lead to even better results. Here the purpose of the
experiments were to motivate the use of reference models in general,
and as we needed to average results over a lot of repetitions per
simulation condition, we preferred such a comparably simple and
computationally fast reference model.


\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

In this paper, we demonstrated the benefits of using a reference model
to improve feature selection, or more generally, model reduction. 
We have motivated and explained the general benefits
of a reference model regardless of the method it is applied in combination
with. Specifically, we have seen how the reference model acts as an
approximation of the data generation mechanism through its predictive
distribution. Such approximation is generally less noisy than the
sample estimation available purely from the observed data, leading to
the main benefits of the reference model approach. In our comparisons,
we have analysed the effect of a reference model in the form of a
filter on the observed target values on top of different widely used
variable selection methods. Overall, using a reference model leads to
more accurate and stable selection results independently of the
specific selection method. These benefits apply to a large family of
different approaches all involving a reference model in one way or the
other. Some of these approaches have been present in the literature
\cite[e.g., see references in][]{vehtari2012survey,paper:projpred}
but often without a clear explanation of {why} they are actually
favourable and how they connect to other related approaches. We hope
that the present paper can fill some of these gaps by providing a
unifying framework and understanding of reference models.

We argue that, whenever it is possible to come up with a reasonable
reference model, it should be employed on top of the preferred
selection procedure or as an integral part of more complex methods,
for example, the projective prediction approach
\citep{paper:projpred}. Note that one of the main challenges in many
real world application will consist in devising a sensible reference
model itself and assessing its predictive performance.
% We did provide
% a generic way to build simple reference models, but those may not
% necessarily be appropriate in all situations.
To build good predictive reference models, which are specifically
tuned to the data and problem at hand, we recommend them to be
developed using a robust Bayesian modelling workflow, for instance, as
outlined by \citet{gelman2013bayesian} and \citet{gabry2019visualization}.

Another main result of this paper is that the projective prediction
approach shows superior performance in minimal subset feature
selection compared to alternative methods whether or not these methods
make use of a reference model. That is, while the reference model is
certainly one important aspect the projective prediction approach, it
is not the only reason for its superior performance.  Rather, by
incoporating the full uncertainty of the posterior predictive
distribution into the feature selection procedure (instead of just
using point estimates) and using principled cross-validation method,
projective predictions combine several desirable features into a
single procedure \citep{paper:projpred}.  In summary, we would
strongly recommend using projective predictions for feature selection
if possible and feasible. However, if this is not an option in a given
situation, we would in any case recommend using a reference model on
top of the chosen feature selection method.

While the projective prediction approach was not designed for the
complete feature selection, the iterative version retains the
excellent false discovery rate property. The iterative projection
predictive approach had lower sensitivity than the other methods, and
it is left for future resarch whether it is possible to improve
sensitivity without sacrifying the good false discovery rate.

All Bayesian models used in the this paper have been implemented in
the probabilistic programming language \texttt{Stan}
\citep{paper:stan} and fit via dynamic Hamiltonian Monte Carlo
\citep{hoffman2014no,betancourt2017conceptual}. The code to run all
the experiments is available on GitHub
(\url{https://github.com/fpavone/ref-approach-paper}).

 
\bibliography{ref_approach}

\appendix

%\hypertarget{ap:shrinkage_signal}{%
%\section{Appendix: Shrinkage factors with Dirichlet-Laplace prior}\label{ap:shrinkage_signal}}
%Figure \ref{fig:k_DL} shows the posterior mean shrinkage factors using the Dirichlet-Laplace prior for the normal means problem in the simulated data study of Section \ref{comparison}. The result is qualitatively equal to the regularised horseshoe prior's one (see Figure \ref{fig:k}). 

%\begin{figure}[tp]
%  \centering
%  \includegraphics[width=0.98\textwidth]{graphics/k_DL.pdf}
%  \caption{Average posterior mean values for the shrinkage factors using the Dirichlet-Laplace prior based on 100 simulations. Respectively in red and in blue relevant and non-relevant variables.\\}
%  \label{fig:k_DL}
%\end{figure}

\section*{Appendix}

As an another example, let us consider a candidate submodel in a
feature selection process and let us refer to the parameter
distribution of the submodel by $\pi$ and to the induced predictive
distribution by $q_{\pi}(\tilde{y})$. We would like to choose $\pi$ so
that $q_{\pi}(\tilde{y})$ maximises some predictive performance
utility, for example, the expected log-predictive density (elpd)
defined as: \
\begin{equation}\label{eq:elpd}
\text{elpd}[q_{\pi}]=\int \text{log}\,q_{\pi}(\tilde{y})p_{t}(\tilde{y})d\tilde{y},
\end{equation}
where $p_{t}(\tilde{y})$ denotes the (usually unknown) true generating
mechanism of future data $\tilde{y}$. If we refer to the posterior
predictive distribution of a reference model with $p(\tilde{y}|D)$,
where $D$ stands for the data on which we conditioned on, we can
approximate \eqref{eq:elpd} using $p(\tilde{y}|D)$ instead of the true
data generation mechanism $p_{t}(\tilde{y})$. The maximisation of the
elpd using the reference model predictive distribution is equivalent
to the minimisation of the Kullback-Leibler (KL) divergence from the
reference model's predictive distribution to the submodel's predictive
distribution: \
\begin{equation} \label{proj_as_filter}
\underset{\pi}{\text{max}} \; \int \text{log}\,q_{\pi}(\tilde{y})p(\tilde{y}|D)d\tilde{y} \leftrightarrow \underset{\pi}{\text{min}} \; \text{KL}[p(\tilde{y}|D)||q_{\pi}(\tilde{y})] 
\end{equation}
The term on the right-hand side of Equation \eqref{proj_as_filter}
describes what is referred to as the projection of the predictive
distribution, which is the general idea behind the projection
predictive approach \cite[see][]{paper:projpred}. Again the reference
model is acting as a filter, or substitute, on data.


Relying on the idea of the reference model as a noise-filter, a
generic reference model approach, which can be used on top of any
feature selection procedure, consists of simply substituting the
target observations with corresponding point predictions of the
reference model, for instance, the posterior predictive
means. Afterwards, any method for feature selection can be used. We do
not argue that such a reference model approach should necessarily be
used as is for feature selection in real applications. There are
several more sophisticated reference model approaches available in the
literature \cite[see,
e.g.][]{paper:projpred,paananen2017variable,paul2008preconditioning},
which are likely to yield even better results. Rather, the reason for
focusing on this simple version is to enable a fair comparison between
a reference model and a purely data based approach when otherwise
using the same selection procedures. Other kinds of comparisons, as
the one presented in Section \ref{intro-example}, work well as
motivational examples but lack in terms of generality since the
presence or absence of a reference model is not the only aspect
differentiating the two procedures.
% We will come back to the more
% general comparison in Section \ref{comparison}.

As a motivational example to the implications of using a reference
model in feature selection problems, we show its application in the
form of the projection predictive approach, which uses the reference
model to project the posterior draws to the submodels
\citep{paper:original_proj}. The \texttt{R}-package \texttt{projpred}
(available at \url{https://CRAN.R-project.org/package=projpred})
implements the projection in case of submodels in the family of the
generalised linear models and, additionally, it provides a framework
to choose the optimal submodel size through a predictive utility
comparison between the submodels and the reference model.

We now summarise the workflow of the projection predictive approach in
the particular case of the draw-by-draw projection \cite[original
formulation by][]{paper:original_proj}, following
\cite{paper:projpred}. Suppose we have observed $n$ statistical units
with target values $\{y_{i}\}_{i=1}^{n}$ and a set of observed
covariates for which we want to obtain a minimally relevant
subset. Than, the main steps are the following:
\begin{enumerate}
\item Devise and fit a reference model. Let $\{\boldsymbol{\theta}_{*}^{s}\}_{s=1}^{S}$ be the set of $S$ draws from the reference model posterior.
\item Rank the covariates according to their relevance using some
  heuristics and consider as candidate submodels only those which
  preserve this order, starting from including only the highest ranked
  covariate (the submodels are then naturally identified by their
  model size). This is not strictly necessary but reduces the number
  of submodels to consider in the following steps.
\item For each submodel, project the reference model posterior draws as follows:
\begin{equation}\label{eq:draw_by_draw}
\boldsymbol{\theta}_{\perp}^{s} = \underset{\boldsymbol{\theta^{s}}\in\Theta}{\text{argmin}} \frac{1}{n}\sum_{i=1}^{n}\text{KL} \left( p(\tilde{y}_{i}|\boldsymbol{\theta}_{*}^{s})\;||\;q(\tilde{y}_{i}|\boldsymbol{\theta}^{s}) \right), \quad s=1,\ldots,S \,,
\end{equation}
where $p(\tilde{y}_{i}|\boldsymbol{\theta}_{*}^{s})$ stands for the
predictive distribution of the reference model with parameters fixed
at $\boldsymbol{\theta}_{*}^{s}$ and conditioning on all the covariate
values related to the statistical unit (identified by the subscript
$i$), whereas $q(\tilde{y}_{i}|\boldsymbol{\theta}^{s})$ is the
predictive distribution of the submodel. The projected draws
$\boldsymbol{\theta}_{\perp}^{s}$ then present the projected posterior
for the submodel.
\item For each submodel (size), test the predictive performance, for example, via cross-validation.
\item Choose the smallest submodel (size) that is sufficiently close to the reference model's predictive utility score.
\end{enumerate} 

Expression \eqref{eq:draw_by_draw} is not an easy optimisation problem
in general, but in the special case of submodels for the family of
generalised linear models, \eqref{eq:draw_by_draw} reduces to a
maximum likelihood estimation problem, which can be easily optimised
\citep{paper:original_proj}. For further details on the projective
prediction workflow and implementation see \cite{paper:projpred}.

\end{document}







