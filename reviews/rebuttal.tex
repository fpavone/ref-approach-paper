% LaTeX rebuttal letter example. 
% 
% Copyright 2019 Friedemann Zenke, fzenke.net
%
% Based on examples by Dirk Eddelbuettel, Fran and others from 
% https://tex.stackexchange.com/questions/2317/latex-style-or-macro-for-detailed-response-to-referee-report
% 
% Licensed under cc by-sa 3.0 with attribution required.
% See https://creativecommons.org/licenses/by-sa/3.0/
% and https://stackoverflow.blog/2009/06/25/attribution-required/

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
%\usepackage{lipsum} % to generate some filler text
\usepackage{fullpage}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[authoryear,round]{natbib}


% import Eq and Section references from the main manuscript where needed
% \usepackage{xr}
% \externaldocument{manuscript}

% package needed for optional arguments
\usepackage{xifthen}
% define counters for reviewers and their points
\newcounter{reviewer}
\setcounter{reviewer}{0}
\newcounter{point}[reviewer]
\setcounter{point}{0}

% This refines the format of how the reviewer/point reference will appear.
\renewcommand{\thepoint}{P\,\thereviewer.\arabic{point}} 

% command declarations for reviewer points and our responses
\newcommand{\reviewersection}{\stepcounter{reviewer} \bigskip \hrule
                  \section*{Reviewer \thereviewer}}
                
\newenvironment{point}
   {\refstepcounter{point} \bigskip \noindent {\textbf{Reviewer~Point~\thepoint} } ---\ }
   {\par }

\newcommand{\shortpoint}[1]{\refstepcounter{point}  \bigskip \noindent 
	{\textbf{Reviewer~Point~\thepoint} } ---~#1\par }

\newenvironment{comment}
{\medskip \noindent \begin{sf}\textbf{Reviewer~comment}:\  }
	{\medskip \end{sf}}

\newenvironment{reply}
   {\medskip \noindent \begin{sf}\textbf{Reply}:\  }
   {\medskip \end{sf}}

\newcommand{\shortreply}[2][]{\medskip \noindent \begin{sf}\textbf{Reply}:\  #2
	\ifthenelse{\equal{#1}{}}{}{ \hfill \footnotesize (#1)}%
	\medskip \end{sf}}

\newcommand{\todo}{\subsection*{\textcolor{red}{To do (comment out when done)}:}}

\newcommand{\fede}[1]{\textcolor{red}{[Fede: #1]}}
%\newcommand{\av}[1]{\textcolor{blue}{[Aki: #1]}}
%\newcommand{\ami}[1]{\textcolor{brown}{[Alonzo: #1]}}
%\newcommand{\ts}[1]{\textcolor{purple}{[Tuomas: #1]}}

\begin{document}

\section*{Response to the reviewers}
% General intro text goes here
We thank the editor and the reviewers for careful review of the paper and the many useful comments.
\bigskip 
\hrule
\medskip 
\section*{Associated Editor}

\paragraph{Associated Editor comment}
Please note the strong concerns of the reviewers. If you cannot implement a better projective prediction approach for complete variable selection problems, then at the very least, I would expect you to motivate why it's worth knowing about this approach; you show that it doesn't work very well.

\paragraph{Reply}
\fede{We have elaborated a better version of our paper following many of the reviewers' comments and suggestions. However, we believe that there has been a misunderstanding on the purpose of our work by Reviewer 1. The projective prediction approach for complete variable selection is not the aim of this work. The method has been included for the sake of a more exhaustive comparison, since the good performances of the projective prediction method (projpred) for minimal variable selection. Indeed, we have highlighted how projpred has been designed specificly for the minimal selection case, making it not trivial to extend for the complete selection. We do not exclude that better projective prediction method for complete selection can be designed in the future. \\
Reviewer 3 gave precious comments on how to clarify the real intentions of our work, which is the study of the effect of reference models when used to guide the selection process, regardless of the selection method. We followed the suggestions and we believe that the current version reduces the possibility of misunderstandings.}

%\todo
%\begin{description}
%    \item [Mans] Nothing
%\end{description}

% Let's start point-by-point with Reviewer 1
\reviewersection
\begin{comment}
The paper is well written and organized an the topic analyzed is very relevant and practice. However, in my view, the goals of the paper, described in the introduction section, have already been reached in the excellent paper of Pironen et al. (2020) and, in this sense, the contributions of the current paper is very marginal. The framework of Pironen et al. (2020) is more general (they consider exponential family models and generalized linear models) and provide some theoretical results that, as they said “... help us to understand when the reference model could be helpful for parameter learning in linear submodels” (section 8 of Piironen et al., 2020). The only innovation that I can appreciate in the current paper is the proposal of an algorithm to solve a complete variable selection problem that, unfortunately, does not work very well as the authors explicitly recognize: “The great performance of projpred in minimal subset selection is not carried out over for the complete variable selection with iterative projpred ..., and the methods specifically designed for the complete variable selection perform better” (page 12, lines 24-27), even though they observe a better selection to iterative lasso. In my view, this is due to that the algorithm is too simple (as the authors explicitly recognize in the conclusions) and that the projective prediction approach was not designed for the complete variable selection. You should modify this algorithm in such a way that a more exhaustive exploration of the space of models was carried out, perhaps using Bayesian variable selection procedures based on the use of Occam’s window or something similar.
\end{comment}

\begin{reply}
	\fede{Thank you for your comments. We believe that in the first version of the paper the purpouse of our work was not clear and subject to possible misunderstanding. Our contribution takes clear insipiration from the work of \citet{piironen2020projective}, but aims at a different goal. We studied what is the action of the reference model itself, disentangled from the specific variable selection algorithm.
	We made it clearer in Section 1, where we now write}
	
	\vspace{0.5cm}
	
	\textit{``The goal of our work is to study the impact of reference model approaches disentangling the benefit of the usage of a reference model from the specific selection algorithm. Specifically, we:
		\begin{itemize}
			\item disentangle the action of the reference model through a simple and intuitive way to combine any variable selection method with the reference model approach. This allows us to build up a comparison independent of the specific variable selection method;
			\item perform extensive numerical experiments to compare variable selection methods with or without using a reference model, both for complete and minimal subset feature selection and assessing the quality of the selection;
			\item provide evidence supporting, in particular, the projection predictive approach as a principled way to use reference models in variable selection.''
		\end{itemize}}
		
		\vspace{0.5cm}
		
		\fede{Moreover, the projective prediction approach for complete variable selection is not the aim of this work. The method has been included for the sake of a more exhaustive comparison, since the good performances of the projective prediction method (projpred) for minimal variable selection. Indeed, we have highlighted how projpred has been designed specificly for the minimal selection case, making it not trivial to extend for the complete selection. We do not exclude that better projective prediction method for complete selection can be designed in the future.}
\end{reply}

% Point one description 
\begin{point}
My main concern with respect to the projective predictive approach is the robustness of the results to the reference model. Apparently, you can build very different references models to the same data set: have you study the sensitivity of the results with different reference models?. You use Bayesian models as reference models and, in these cases, the determination of prior distribution is not very clear. You can build hundreds and hundreds of prior distribution: for instance in (7) you can have use standard inverted gamma priors on $\tau^2$ and $\sigma^2$ by determining the parameters of the prior in order to the prior mixture gives reasonable results about the data using the prior mixture. How can you study the robustness of your results to this important aspect?
\end{point}

\begin{reply}
Thank you for your comments. We have added clarification at the end of Section 2.3 regarding the reference model, in particular we write that

\vspace{0.5cm}
\textit{
``Clearly, the reference model approach requires a sensible model. The construction of such a model should follow proper modelling workflow \citep[see, e.g.][]{gelman2020bayesian}. Better predictive models imply better selection results. The goodness of a reference model comes from its predictive ability which should be assessed via proper validation methods. In general, there is not restriction on the type of model the reference models should be and a sensible reference model does not need to be Bayesian.''
}
\vspace{0.5cm}

So that for example a model predicting constant output or a model predicting uncorrelated noise are not useful reference models.

The sensitivity of the Bayesian models to choice of priors is well studied and there exists well-developed workflows for doing prior sensitivity analysis and other model checking \citep[see, e.g.][]{gelman2020bayesian}. In this paper the point is not the Bayesian workflow, but to analyse given a good reference model what is the importance of making the submodel inference using the decision theoretically justified version or whether other algorithms can also be improved by replacing the data with filtered prediction from the reference model. 

The reference models used in the examples are using widely accepted weakly informative priors and there is no prior sensitivity issues. As the same reference models are used for all algorithms, the relative comparison between different algorithms and the overall usefulness of the reference model approach is not influenced by the details of the reference models.
\end{reply}


%\todo
%\begin{description}
%    \item[Mans] Clarified that we present first time how this problem should be approached in the Bayesian context.
%\end{description}

\begin{point}
Perhaps a solution to the previous problem would be to determine the reference model using frequentist MLE procedures that avoid the construction of prior distributions and use a single point prediction or, alternatively, to use the sampling distribution of MLE as a posterior distribution and take this “Bayesian” model as reference model. Do you have any experience in this line?
\end{point}

\begin{reply}
MLE is more unstable than Bayesian approach. The Bayesian approach gets the improved stability both from integration over the parameter space (instead of using a point estimate) and can further be improved by using even just weakly informative priors. We have added references that discuss merits of Bayesian approach compared to MLE \fede{not yet added!}. Although we think Bayesian reference models are better choice, we also assume that in large n, small p case there will be not much difference to using MLE to construct the reference model, and we believe that in what ever way the reference model has been built, if it's a good predictive model, it will be a useful reference model.
\end{reply}


\begin{point}
What if the true generator process is a mixture model, in such a way that there is not a unique solution to the minimal subset variable selection problem? In fact, how can you assure that the minimal subset variable selection problems have only one selection? I think that the algorithm described in appendix A should take into account this possibility, and the step 5 not to choose the smallest (model) that is sufficient close to the reference model's predictive utility score but to use, for instance, an Occam window, that reflects more adequately the uncertainty associated with the model selection process something that, apparently, your methodology does not consider. In fact, and even though in your examples your results in the paper were not sensitive to the specific choice of how “sufficiently close” is defined, I guess that there can be situations where this statement is not true, and more of one solution to the minimal subset variable selection problem and to the complete variable selection problem could exist.
\end{point}

\begin{reply}
The projection predictive variable selection and its properties have been extensively discussed in \citet{piironen2017comparison} and \citet{piironen2020projective}, and we refer to them for the detailed discussion on the uncertainty related to search paths. \citet{paasiniemi2018} provides illustrations how this uncertainty can be communicated to the users. When there are correlating variables that have exactly the same amount of information, the minimal set is not unique. In this paper, we focused on the average performance of the minimal set, but when analysing a single dataset, we recommend the approaches presented in \citet{paasiniemi2018} to analyse alternative possible variable sets.
\end{reply}

\begin{point}
In most of real situations, we have an M-open problem (particularly in big data problems) where the true generator mechanism is too complex and it is not included in the family of considered models. What would be the performance of your methodology if the reference model is far from the true generator process? How can you detect these situations?
\end{point}

\begin{reply}
We assume that all models are wrong. Analysing model misspecification is well studied in Bayesian literature and there are plenty of methods available for model checking \citep[see, e.g.][]{gelman2020bayesian}. The reference model approach assumes we have first made the best possible model, following all the best practices for Bayesian model building \citep{gelman2020bayesian}. Even a misspecifed model that would introduce some bias can reduce the variance so that the variable selection is improved. 
\end{reply}

\begin{point} \label{q5}
In big data problems with $n<<p$ and the explanatory variables are weakly related (in such a way that the number of significant principal components is large), how do you build a reference model? Is it possible?
\end{point}

\begin{reply}
Please, see \citet{piironen2020projective} for further information.
\end{reply}

\begin{point}
The same question that \ref{q5} but if n is small and your data have not power to discriminate between two alternative reference models.
\end{point}

\begin{reply}
The usual Bayesian workflow \citep{gelman2020bayesian} states that if there is uncertainty about which model then we integrate over the model space or create a continuous super model that includes the alternative models as special cases.
\end{reply}

%%% SOME OF THE MINOR REQUEST
\section*{Reviewer 1 - Minor comments}

\begin{point}
Section 3.1: you say that in this simulation you use a Bayesian stepwise forward selection procedure. Is this really the case? In your description of the procedure in page
6 you exclude (and not include variables) in this step of the algorithm. In my view, this procedure is a Bayesian stepwise backward selection procedure.
\end{point}

\begin{reply}
Thanks for pointing out the typo. We fixed it in the new version of the paper.
\end{reply}

\begin{point}
In the Bayesian stepwise selection procedures of section 3.1 you use posterior pvalues to exclude variables, and you exclude the variable with the highest Bayesian p- value. Shouldn't you take into account whether the value of the pvalue is large or small? Shouldn't you calibrate these pvalues?
\end{point}

\begin{reply}
\fede{There is no need of calibration of the pvalues, since they are used only to select the "worse" predictor. Such predictor is then excluded or not, looking at the predictive performance of the reduced model and of the not-reduced one. In such a way, the posterior pvalues matter only for their ordering and not for their magnitude.}
\end{reply}

\begin{point}
Section 3.1 steplm procedure: Why have you chosen the AIC criterion which, as it is well-known tend to select non-parsimonious models? Have you tried to use the BIC criterion instead? In my experience, this criterion tend to select more parsimonious models and in a M Complete approach, it is usually consistent.
\end{point}

\begin{reply}
We used AIC because reference \citet{heinze2018variable} was using AIC. The goal of the paper was not to compare all possible information criteria. We made choices to keep the paper within reasonable length. Based on the results, we assume that variable selection using any *IC approach will benefit from using a reference model. 
\end{reply}

\begin{point}
Section 4.1, page 8, line 38: $\text{elpd}_\text{base}$ should be $\text{elpd}_\text{best}$
\end{point}

\begin{reply}
Thanks for pointing out the typo. We fixed it in the new version of the paper.
\end{reply}

\begin{point}
How is calculated the probability used in the stopping rule (2)? Using the reference model?
\end{point}

\begin{reply}\label{reply:stopping}
\fede{At each iteration, the best model is selected via the default stopping rule implemented in the \texttt{R}-package \texttt{projpred}. For details, see \citet{piironen2020projective}. In few words, the predictive performance is evaluated via cross-validation both for the reference model and the submodels using the log-predictive density (lpd) score. The obtained lpd samples are used to estimate that probability.}
\end{reply}

\begin{point}
It is not very exigent in (2) to impose that $\text{elpd}_i-\text{elpd}_\text{best}>0$? Perhaps you should consider other limits similar to those used in Bayes factors (Kass and Raftery, 1995) and not to choose only on solution but to explore other possible solutions in the spirit of Occam’s window
\end{point}

\begin{reply}
\fede{The above condition derives from the original criteria to choose the model size in \citet{piironen2020projective} for the use of the projection predictive approach in the minimal subset scenario. In that case, the comparison is between the predictive score of the submodel of size $i$ and the reference model. Here we decided to compare the submodel of size $i$ with the ``best'' size, at the given iteration, due to the natural loss of explanatory power of the remaining covariates as the iterations increase. \\
In general, notice that the condition is expressed as the probability of that event, which is anyhow less exigent than the inequality itself.}
\end{reply}

\begin{point}
Section 4.1: It is no clear how the value of $\alpha$ should be chosen. $\alpha= 0.16$ is a magic number and a more thoroughly discussion of the influence of this value should be provided.
\end{point}

\begin{reply}
\fede{The choice of $\alpha$ to determine the submodel size is discussed in \citet{piironen2020projective}. One possible way to proceed, but just as rule of thumb, is to choose $\alpha=0.16$, which correspond to requiring that the submodel predictive score is at most at one standard deviation distance from the best submodel predictive score.}
\end{reply}

\begin{point}
Section 4.1, line 50 “expect” should be “except”
\end{point}

\begin{reply}
Thanks for pointing out the typo. We fixed it in the new version of the paper.
\end{reply}

\reviewersection

No comments provided.

\reviewersection
%%%% REVIEWER 3

\begin{point}
Although main articles related to the use of the reference model approach in the model selection are mentioned, the motivation of the study in relation to the literature is missing in the Introduction. The given discussion of the existing literature supports the idea that the reference models have been used in Bayesian and non-Bayesian contexts. However, why do we need the proposed approach in the article is missing. Full motivation is given in Section 2. However, it needs to be briefly mentioned in the Introduction as well to motivate the reader to keep reading. In addition, instead of pointing to the references, please give a brief critique of them.
\end{point}

\begin{reply}
\fede{Thank you for the feedback. We renovated Section 1, improving the paper according to your suggestions. In particolar, we clarified what is the motivation of our work and, thus, what is the purpouse of the reference model approach used in our experiments. In the new version of the paper we write
}
\vspace{0.5cm}

\textit{``The goal of our work is to study the impact of reference model approaches disentangling the benefit of the usage of a reference model from the specific selection algorithm. Specifically, we:
	\begin{itemize}
		\item disentangle the action of the reference model through a simple and intuitive way to combine any variable selection method with the reference model approach. This allows us to build up a comparison independent of the specific variable selection method;
		\item perform extensive numerical experiments to compare variable selection methods with or without using a reference model, both for complete and minimal subset feature selection and assessing the quality of the selection;
		\item provide evidence supporting, in particular, the projection predictive approach as a principled way to use reference models in variable selection.''
	\end{itemize}}
	
	\vspace{0.5cm}	

\end{reply}

\begin{point}
Please include the computational cost of compared approaches in Section 2.1. 
\end{point}

\begin{reply}
\fede{We touch the computational aspects in Section 2.2. The reference model approach has in general the additional cost of fitting a reference model. However, this is usually available when building up a model via robust workflows \citep{gelman2020bayesian}. Therefore, in these cases there is not additional cost. In any case we believe that the benefits of a more stable and efficient selection overcome the cost of fitting a reference model.}
\end{reply}

\begin{point}
Please add a brief summary of the results you obtained in Section 3 and the main message you infer from these results before moving onto Section 4.
\end{point}

\begin{reply}
\fede{We added a new Section 3.3 which summarises and elaborate the results of Section 3. In particular, we write:
}

\vspace{0.5cm}
\textit{
The previous experiments allowed us to study and disantangle the effect of a reference model in variable selection, specifically in the minimal subset selection case.
The simulation study based on artificial data of Section 3.1 shows clear improvements in terms of predictive performance of the selected model and false discovery rate when the selection is based on a reference model, regardless of the method applied. These results are confirmed also in the real word data example with the body fat dataset. The stepwise selection achieves far better selection with the reference model approach (see Figure 5 and Table 3). However, the projection predictive approach remains the best method in any of the experiment we run and on all the performance indexes we measured. Although we designed a reference model approach for general selection method, the projection predictive approach is a principled and validated way to do the selection. Indeed, the purpose of the former is only for fair comparisons in our study rather than a ready-to-use selection method.
}
\vspace{0.5cm}
\end{reply}

\begin{point}
Please mention the availability of the R codes to reproduce the results under each section as well.
\end{point}

\begin{reply}
Thank you for the feedback, we mentioned the availability of the R codes under each section in the new version of the paper.
\end{reply}


\bibliographystyle{abbrvnat}
\bibliography{rebuttal}

\end{document}