% LaTeX rebuttal letter example. 
% 
% Copyright 2019 Friedemann Zenke, fzenke.net
%
% Based on examples by Dirk Eddelbuettel, Fran and others from 
% https://tex.stackexchange.com/questions/2317/latex-style-or-macro-for-detailed-response-to-referee-report
% 
% Licensed under cc by-sa 3.0 with attribution required.
% See https://creativecommons.org/licenses/by-sa/3.0/
% and https://stackoverflow.blog/2009/06/25/attribution-required/

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
%\usepackage{lipsum} % to generate some filler text
\usepackage{fullpage}
\usepackage{xcolor}
\usepackage[pdftex,colorlinks=true,citecolor=black,pagecolor=black,linkcolor=black,menucolor=black, urlcolor=black]{hyperref}
\usepackage{amsmath}
\usepackage[authoryear,round]{natbib}


% import Eq and Section references from the main manuscript where needed
% \usepackage{xr}
% \externaldocument{manuscript}

% package needed for optional arguments
\usepackage{xifthen}
% define counters for reviewers and their points
\newcounter{reviewer}
\setcounter{reviewer}{0}
\newcounter{point}[reviewer]
\setcounter{point}{0}

% This refines the format of how the reviewer/point reference will appear.
\renewcommand{\thepoint}{P\,\thereviewer.\arabic{point}} 

% command declarations for reviewer points and our responses
\newcommand{\reviewersection}{\stepcounter{reviewer} \bigskip \hrule
                  \section*{Reviewer \thereviewer}}
                
\newenvironment{point}
   {\refstepcounter{point} \bigskip \noindent \begin{sf}{\textbf{Reviewer~Point~\thepoint} } ---\ }
   {\par \end{sf}}

 \newcommand{\shortpoint}[1]{\refstepcounter{point}  \bigskip \noindent
   \begin{sf}
     {\textbf{Reviewer~Point~\thepoint} } ---~#1\par
   \end{sf}}

\newenvironment{comment}
{\medskip \noindent \begin{sf}\textbf{Reviewer~comment}:\  }
	{\medskip \end{sf}}
      
\newenvironment{aecomment}
{\medskip \noindent \begin{sf}\textbf{Associate Editor comment}:\  }
	{\medskip \end{sf}}

\newenvironment{reply}
   {\medskip \noindent \textbf{Reply}:\  }
   {\medskip}

\newcommand{\shortreply}[2][]{\medskip \noindent \begin{sf}\textbf{Reply}:\  #2
	\ifthenelse{\equal{#1}{}}{}{ \hfill \footnotesize (#1)}%
	\medskip \end{sf}}

\newcommand{\todo}{\subsection*{\textcolor{red}{To do (comment out when done)}:}}

\newcommand{\fede}[1]{\textcolor{red}{[Fede: #1]}}
%\newcommand{\av}[1]{\textcolor{blue}{[Aki: #1]}}
%\newcommand{\ami}[1]{\textcolor{brown}{[Alonzo: #1]}}
%\newcommand{\ts}[1]{\textcolor{purple}{[Tuomas: #1]}}

\begin{document}

\section*{Response to the reviewers}
% General intro text goes here
We thank the editor and the reviewers for careful review of the paper and the many useful comments.
\bigskip 
\hrule
\medskip 
\section*{Associate Editor}

\begin{aecomment}
Please note the strong concerns of the reviewers. If you cannot implement a better projective prediction approach for complete variable selection problems, then at the very least, I would expect you to motivate why it's worth knowing about this approach; you show that it doesn't work very well.
\end{aecomment}

\begin{reply}
Based on the reviewers' comments and suggestions, we have substantially improved our paper. In particular, we hope that we now better explain the purpose of our work, which was a source of confusion in our original version.

We hope that this specifically clarifies one misunderstanding on the purpose of our work highlighted by Reviewer 1 as we now better explain that the projective prediction approach for complete variable selection is not the main aim of the paper. This method has been included solely for the sake of a more exhaustive comparison, because of the good performances of the projective prediction method (projpred) for minimal variable selection. Indeed, we have highlighted how projpred has been designed specifically for the minimal selection case, making it non-trivial to extend for the complete selection. We cannot rule out that better projective prediction methods for complete selection can be designed in the future but this is not in the scope of this paper. 

Reviewer 3 gave precious comments on how to clarify the real intentions of our work, which is the study of the effect of reference models when used to guide the selection process, regardless of the selection method. We followed the suggestions, and we believe that the current version reduces the possibility of misunderstandings.
\end{reply}

%\todo
%\begin{description}
%    \item [Mans] Nothing
%\end{description}

% Let's start point-by-point with Reviewer 1
\reviewersection
\begin{comment}
The paper is well written and organized an the topic analyzed is very relevant and practice. However, in my view, the goals of the paper, described in the introduction section, have already been reached in the excellent paper of Piironen et al. (2020) and, in this sense, the contributions of the current paper is very marginal. The framework of Piironen et al. (2020) is more general (they consider exponential family models and generalized linear models) and provide some theoretical results that, as they said “... help us to understand when the reference model could be helpful for parameter learning in linear submodels” (section 8 of Piironen et al., 2020). The only innovation that I can appreciate in the current paper is the proposal of an algorithm to solve a complete variable selection problem that, unfortunately, does not work very well as the authors explicitly recognize: “The great performance of projpred in minimal subset selection is not carried out over for the complete variable selection with iterative projpred ..., and the methods specifically designed for the complete variable selection perform better” (page 12, lines 24-27), even though they observe a better selection to iterative lasso. In my view, this is due to that the algorithm is too simple (as the authors explicitly recognize in the conclusions) and that the projective prediction approach was not designed for the complete variable selection. You should modify this algorithm in such a way that a more exhaustive exploration of the space of models was carried out, perhaps using Bayesian variable selection procedures based on the use of Occam’s window or something similar.
\end{comment}

\begin{reply}
	Thank you very much for your comments. We believe that in the first version of the paper, the purpose of our work was not clear and subject to possible misunderstandings. Our contribution takes clear inspiration from the work of \citet{paper:projpred}, but aims at a different goal: We studied the impact of using a reference model in general, disentangled from the specific variable selection algorithm.
	We now make our goals explicit already in Section 1, where we now write
	
\vspace{0.5\baselineskip}
	\textit{The goal of the present paper is to study the impact of reference model approaches by disentangling the benefit of using reference models per se from the benefit of specific variable selection algorithms. In particular, we:
		\begin{itemize}
			\item propose a simple and intuitive approach to combine any variable selection method with the reference model approach. This allows us to investigate the benefit of using reference models independent of the specific variable selection method;
			\item perform extensive numerical experiments to compare variable selection methods with or without using a reference model, both for complete and minimal subset variable selection and assessing the quality of the selection;
			\item provide evidence supporting, in particular, the projection predictive approach as a principled way to use reference models in minimal subset variable selection.
		\end{itemize}}
		
		\vspace{0.5\baselineskip}
		
		Moreover, we clarified that the projective prediction approach for complete variable selection is not the main aim of this work. The method has been included solely for the sake of a more exhaustive comparison, since the good performances of the projective prediction method (projpred) for minimal variable selection. Indeed, we have highlighted how projpred has been designed specifically for the minimal selection case, making it non-trivial to extend for the complete selection. We cannot rule out that better projective prediction methods for complete selection can be designed in the future but this is not in the scope of this paper. 
\end{reply}

% Point one description 
\begin{point}
My main concern with respect to the projective predictive approach is the robustness of the results to the reference model. Apparently, you can build very different references models to the same data set: have you study the sensitivity of the results with different reference models?. You use Bayesian models as reference models and, in these cases, the determination of prior distribution is not very clear. You can build hundreds and hundreds of prior distribution: for instance in (7) you can have use standard inverted gamma priors on $\tau^2$ and $\sigma^2$ by determining the parameters of the prior in order to the prior mixture gives reasonable results about the data using the prior mixture. How can you study the robustness of your results to this important aspect?
\end{point}

\begin{reply}
Thank you for this comment. We have added some clarifications at the end of Section 2.2 with regard to building the reference model, where we write

\vspace{0.5\baselineskip}
\textit{
Clearly, the reference model approach requires a sensible reference model. The construction of such a model should follow proper modelling workflow \citep[see, e.g.][]{gelman2020bayesian}. Better predictive models imply better selection results. The goodness of a reference model comes from its predictive ability which should be assessed via proper validation methods. [...] In general, there is no restriction on the type of model the reference models should be, and a sensible reference model does not even need to be Bayesian necessarily.
}
\vspace{0.5\baselineskip}

As such, for example, a model predicting constant output or a model predicting uncorrelated noise are not useful reference models.

The sensitivity of the Bayesian models to choice of priors is well studied, and there exists well-developed workflows for doing prior sensitivity analysis and other model checking \citep[see, e.g.][]{gelman2020bayesian}. As such, the aim of this paper is not to show how to build good reference models. Instead, we study the importance of reference models in submodel inference using either the decision theoretically justified version or other algorithms that we show can also be improved by replacing the data with filtered prediction from the reference model. 

The reference models applied in the examples are using widely accepted weakly informative priors, and there are no prior sensitivity issues for them. As the same reference models are used for all algorithms, the relative comparison between different algorithms and the overall usefulness of the reference model approach is not influenced by the details of the reference models.
\end{reply}


%\todo
%\begin{description}
%    \item[Mans] Clarified that we present first time how this problem should be approached in the Bayesian context.
%\end{description}

\begin{point}
Perhaps a solution to the previous problem would be to determine the reference model using frequentist MLE procedures that avoid the construction of prior distributions and use a single point prediction or, alternatively, to use the sampling distribution of MLE as a posterior distribution and take this “Bayesian” model as reference model. Do you have any experience in this line?
\end{point}

\begin{reply}
This is an interesting question. The way we see it is that MLE tends to be more unstable than comparable Bayesian approaches, as the latter get improved stability both from integration over the parameter space (instead of using only a point estimate), and can further be improved by using even just weakly informative priors (rather than improper flat priors). We have added references that discuss the merits of Bayesian approaches compared to MLE. Although we think Bayesian reference models are a better choice in most cases, we also assume that in large $n$, small $p$ cases there will be not much difference to using MLE to construct the reference model, and we believe that in what ever way the reference model has been built, if it's a good predictive model, it will be a useful reference model too.

Inspired by your question, we added a discussion of this in the new version of the paper. In Section 2.2 we write

\vspace{0.5\baselineskip}
\textit{In general, there is no restriction on the type of model the reference models should be and a sensible reference model does not even need to be Bayesian necessarily.
	However, Bayesian methods can help in some of those situations
	where MLE procedures struggle. 
	It is known that, as the number of parameters in the model increases, the MLE
	estimator is dominated by shrinkage estimators 
	\citep{stein1956inadmissibility, stein1961estimation, parmigiani2009decision, efron2011tweedie}.
	The use of a prior in Bayesian inference automatically incorporates 
	some kind of shrinkage in the Bayesian estimator under a given loss function \citep[see, e.g.][]{rockova2012hierarchical}. 
	For the sake of our study, we rely on simple data structure which can be well be described 
	by linear regression models. However, more complex data can arise in practice, for example, including hierarchical structures. In these cases, MLE inference tends to become cumbersome, whereas the Bayesian framework provides a natural way to convey uncertainties and make inference through the joint posterior distribution of all parameters.}

\vspace{0.5\baselineskip}

\end{reply}


\begin{point}
What if the true generator process is a mixture model, in such a way that there is not a unique solution to the minimal subset variable selection problem? In fact, how can you assure that the minimal subset variable selection problems have only one selection? I think that the algorithm described in appendix A should take into account this possibility, and the step 5 not to choose the smallest (model) that is sufficient close to the reference model's predictive utility score but to use, for instance, an Occam window, that reflects more adequately the uncertainty associated with the model selection process something that, apparently, your methodology does not consider. In fact, and even though in your examples your results in the paper were not sensitive to the specific choice of how “sufficiently close” is defined, I guess that there can be situations where this statement is not true, and more of one solution to the minimal subset variable selection problem and to the complete variable selection problem could exist.
\end{point}

\begin{reply}
The projection predictive variable selection and its properties have been extensively discussed in \citet{piironen2017comparison} and \citet{paper:projpred}, and we refer to them for the detailed discussion on the uncertainty related to search paths. \citet{paasiniemi2018} provides illustrations how this uncertainty can be communicated to the users. When there are correlating variables that have exactly the same amount of information, the minimal set is not unique. In this paper, we focused on the average performance of the minimal set, but when analysing a single dataset, we recommend the approaches presented in \citet{paasiniemi2018} to analyse alternative possible variable sets.

We have clarified this point in the paper in Section 3, writing:

\vspace{0.5\baselineskip}

\textit{Such nature of the minimal subset makes the solution
	not unique, except in the particular case of completely orthogonal predictors. 
	The non-uniqueness of the solution is not a problem per se, as different samples of the data
	are expected to give possible different minimal subsets but with same predictive power. 
	However, it makes it difficult to define a proper
	concept of stability of the selection.}
\end{reply}

\begin{point}
In most of real situations, we have an M-open problem (particularly in big data problems) where the true generator mechanism is too complex and it is not included in the family of considered models. What would be the performance of your methodology if the reference model is far from the true generator process? How can you detect these situations?
\end{point}

\begin{reply}
We assume that all models are wrong. Analysing model misspecification is well studied in the Bayesian literature, and there are plenty of methods available for model checking \citep[see, e.g.][]{gelman2020bayesian}. The reference model approach assumes we have first made the best possible model, following all the best practices for Bayesian model building \citep{gelman2020bayesian}. Even a misspecified model that would introduce some bias can reduce the variance so that the variable selection is improved via an improved bias-variance tradeoff. As the selection is to find a submodel that is not far from the reference model, the performance of the selected model is not far from the reference model.

We have clarified this in the introduction:
\end{reply}

\vspace{0.5\baselineskip}
\textit{
We assume all models are wrong, but we assume we have constructed a
model which reflects our beliefs about the future data in the best
possible way and which has passed model checking and criticism
\citep{gelman2020bayesian,gabry2019visualization}.  Using the usual
best practices for constructing the reference model is important, as
using a bad reference model can only lead to selecting a similarly
bad smaller model. If the reference model is considered to have
useful predictions, then the smaller models selected will also have
similar useful predictions.
}

\begin{point} \label{q5}
In big data problems with $n<<p$ and the explanatory variables are weakly related (in such a way that the number of significant principal components is large), how do you build a reference model? Is it possible?
\end{point}

\begin{reply}
  Yes, it is possible. This situation has been studied by
  \citet{piironen2017comparison},
  \citet{Piironen:2015:Stan_projection}, and
  \citet{paper:projpred}. We had added to the introduction:
\end{reply}


\vspace{0.5\baselineskip}
\textit{
Examples of useful reference models can be found for example for
small-$n$-large-$p$ regression and logistic regression by
\citet{piironen2017comparison} (with spike-and-slab prior),
\citet{Piironen:2015:Stan_projection} (with horseshoe prior), and
\citet{paper:projpred} (with iterative supervised principal
components), for generalized linear and additive multilevel models by
\citet{Catalina:2020:GAMM_projection}, for regression models with
non-exponential family observation models by
\citet{Catalina+etal:2021:latent_projection}, and for generic
multivariate non-linear regression with higher order interactions by
\citet{Piironen:2016:GPprojection}.
}

\begin{point}
The same question that \ref{q5} but if n is small and your data have not power to discriminate between two alternative reference models.
\end{point}

\begin{reply}
Following current best practices in the Bayesian workflow \citep{gelman2020bayesian}, if there is uncertainty about which model is best, then we recommend integrating over the model space or creating a continuous super model that includes the alternative models as special cases. 
\end{reply}

%%% SOME OF THE MINOR REQUEST
\section*{Reviewer 1 - Minor comments}

\begin{point}
Section 3.1: you say that in this simulation you use a Bayesian stepwise forward selection procedure. Is this really the case? In your description of the procedure in page
6 you exclude (and not include variables) in this step of the algorithm. In my view, this procedure is a Bayesian stepwise backward selection procedure.
\end{point}

\begin{reply}
Thanks for pointing out the typo. We fixed it in the new version of the paper.
\end{reply}

\begin{point}
In the Bayesian stepwise selection procedures of section 3.1 you use posterior pvalues to exclude variables, and you exclude the variable with the highest Bayesian p- value. Shouldn't you take into account whether the value of the pvalue is large or small? Shouldn't you calibrate these pvalues?
\end{point}

\begin{reply}
There is no need of calibrating these $p$-values, since they are used only to pre-select predictors for potential exclusion. Such a predictor is then excluded or not, by comparing the predictive performance of the reduced model without this predictor and the not-reduced model including the predictor. In such a way, the posterior $p$-values matter only in terms of their ordering, and not in terms of their magnitude.
\end{reply}

\begin{point}
Section 3.1 steplm procedure: Why have you chosen the AIC criterion which, as it is well-known tend to select non-parsimonious models? Have you tried to use the BIC criterion instead? In my experience, this criterion tend to select more parsimonious models and in a M Complete approach, it is usually consistent.
\end{point}

\begin{reply}
We used AIC because \citet{heinze2018variable} used AIC in the steplm procedure, and we wanted our results to be comparable to their results. The goal of our paper was not to compare all possible information criteria but to investigate whether using reference models helps variable selection methods. We made some choices to keep the paper within reasonable length and complexity. Based on the results, we assume that variable selection using any *IC approach will benefit from using a reference model. 
\end{reply}

\begin{point}
Section 4.1, page 8, line 38: $\text{elpd}_\text{base}$ should be $\text{elpd}_\text{best}$
\end{point}

\begin{reply}
Thanks for pointing out the typo. We have fixed it.
\end{reply}

\begin{point}
How is calculated the probability used in the stopping rule (2)? Using the reference model?
\end{point}

\begin{reply}\label{reply:stopping}
At each iteration, the best model is selected via the default stopping rule implemented in the \texttt{R}-package \texttt{projpred}. For details, see \citet{paper:projpred}. The predictive performance is evaluated via cross-validation both for the reference model and the submodels using the expected log-predictive density (elpd) score. The obtained elpd samples form a distribution than can be used to estimate said probability. We have added following sentence:
TODO: is the sentence below enough? Paul: Please note my change to the below sentence.
\end{reply}

\textit{
\ldots and the probability is computed from a normal approximation of the uncertainty in the cross-validation performance comparison using the mean and standard error of the elpd difference between reference and submodel \citep{Vehtari+etal:2017:practical,paper:projpred,Sivula+etal:2020:loo_uncertainty}.
}

\begin{point}
It is not very exigent in (2) to impose that $\text{elpd}_i-\text{elpd}_\text{best}>0$? Perhaps you should consider other limits similar to those used in Bayes factors (Kass and Raftery, 1995) and not to choose only on solution but to explore other possible solutions in the spirit of Occam’s window
\end{point}

\begin{reply}
The above condition is derived from the original criteria to choose the model size in \citet{paper:projpred} for the use of the projection predictive approach in the minimal subset scenario. In that case, the comparison is between the predictive score of the submodel of size $i$ and the reference model. Here we decided to compare the submodel of size $i$ with the ``best'' size, at the given iteration, due to the natural loss of explanatory power of the remaining covariates as the iterations increase. \\
In general, the condition is expressed as the probability of that event, which is anyhow less exigent than the inequality itself. We don't want to imply that this procedure could not be improved somehow to take, for example, something like Occam's window into account. However, since the projection predictive approach was not the main point of this paper, we cannot reasonably address it here, and leave it for future research. We have added a following sentence to the paper.
TODO: would this be enough? Paul: please note that I changed the sentence below
\end{reply}

\vspace{0.5\baselineskip}
\textit{
We have used the stopping rule recommended by \citet{paper:projpred} for the minimal subset selection, but other stopping rules would be possible in the iterative case and may be worth further research.
}

\begin{point}
Section 4.1: It is no clear how the value of $\alpha$ should be chosen. $\alpha= 0.16$ is a magic number and a more thoroughly discussion of the influence of this value should be provided.
\end{point}

\begin{reply}
We have clarified the choice of $\alpha$ by writing in Section 4.1:

\vspace{0.5\baselineskip}

\textit{The choice of $\alpha$ to determine the submodel size is discussed in \citet{paper:projpred}. One possible way to proceed, but just as rule of thumb, is to choose $\alpha=0.16$, which correspond to requiring that the submodel predictive score is at most at one standard deviation distance from the best submodel predictive score.}

\end{reply}

\begin{point}
Section 4.1, line 50 “expect” should be “except”
\end{point}

\begin{reply}
Thanks for pointing out the typo. We have fixed it.
\end{reply}

\reviewersection

No comments provided.

\reviewersection
%%%% REVIEWER 3

\begin{point}
Although main articles related to the use of the reference model approach in the model selection are mentioned, the motivation of the study in relation to the literature is missing in the Introduction. The given discussion of the existing literature supports the idea that the reference models have been used in Bayesian and non-Bayesian contexts. However, why do we need the proposed approach in the article is missing. Full motivation is given in Section 2. However, it needs to be briefly mentioned in the Introduction as well to motivate the reader to keep reading. In addition, instead of pointing to the references, please give a brief critique of them.
\end{point}

\begin{reply}
Thank you for your very helpful feedback. We have updated Section 1 following your suggestions. In particular, we clarified the motivation of our work and, thus, the purpose of the reference model approach used in our experiments. In the revised version of the paper we write

\vspace{0.5\baselineskip}

\textit{The goal of the present paper is to study the impact of reference model approaches by disentangling the benefit of using reference models per se from the benefit of specific variable selection algorithms. In particular, we:
		\begin{itemize}
			\item propose a simple and intuitive approach to combine any variable selection method with the reference model approach. This allows us to investigate the benefit of using reference models independent of the specific variable selection method;
			\item perform extensive numerical experiments to compare variable selection methods with or without using a reference model, both for complete and minimal subset variable selection and assessing the quality of the selection;
			\item provide evidence supporting, in particular, the projection predictive approach as a principled way to use reference models in minimal subset variable selection.
		\end{itemize}}
	
	\vspace{0.5\baselineskip}	

\end{reply}

\begin{point}
Please include the computational cost of compared approaches in Section 2.1. 
\end{point}

\begin{reply}
Thanks for the suggestion. We discuss the computational aspects in Section 2.2. In general, the reference model approach has the additional cost of fitting the reference model before performing selection. However, this reference models is usually available anyway when building up a model via robust workflows \citep{gelman2020bayesian}. Therefore, in these cases there is no additional cost. In any case we believe that the benefits of a more stable and efficient selection overcome the cost of fitting a reference model.
\fede{We added a new table (Table 3) reporting the computational time for the bodyf fat example, showing how the burden is divided between the fit of the reference model and the rest of the selection procedure.}
Paul: why is this still in red?
\end{reply}

\begin{point}
Please add a brief summary of the results you obtained in Section 3 and the main message you infer from these results before moving onto Section 4.
\end{point}

\begin{reply}
This is a good idea. We added a new Section 3.3 which summarises and elaborate the results of Section 3. In particular, we write:

\vspace{0.5\baselineskip}
\textit{
The previous experiments allowed us to study the impact of using reference models in variable selection and to disentangle their influence from that of the actual variable selection method, specifically in the minimal subset selection case.
The simulation study based on artificial data of Section 3.1 shows clear improvements in terms of predictive performance of the selected model and false discovery rate when the selection is based on a reference model, regardless of the actual variable selection method applied. These results are confirmed also in the real word data example with the body fat dataset. The stepwise selection achieves far better selection when coupled with the reference model approach (see Figure 5 and Table 4). However, the projection predictive approach remains the best method in any of the experiments we run and on all the performance indexes we measured. Although we designed a reference model approach for general selection method, the projection predictive approach is a principled and validated way to do the selection. Indeed, the purpose of the former is only for fair comparisons in our study rather than a ready-to-use selection method.
}
\vspace{0.5\baselineskip}
\end{reply}

\begin{point}
Please mention the availability of the R codes to reproduce the results under each section as well.
\end{point}

\begin{reply}
Thank you for the feedback, we mentioned the availability of the R code in the end of the introduction.
\end{reply}


\bibliographystyle{abbrvnat}
\bibliography{../paper/ref_approach}

\end{document}